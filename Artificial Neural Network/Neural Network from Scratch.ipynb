{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xxR-iMl6VXDg"
   },
   "source": [
    "1. The Setup (Single Neuron)For a single neuron with two weights $w_1, w_2$ and a bias $b$\n",
    "Linear Combination ($z$):$$z = w_1x_1 + w_2x_2 + b$$Activation ($a$):$$a = \\sigma(z) = \\frac{1}{1 + e^{-z}}$$Log Loss ($L$):$$L = -(y \\log(a) + (1 - y) \\log(1 - a))$$\n",
    "\n",
    "2. The Chain Rule DerivationTo find the gradient of the loss with respect to the weights, we use the chain rule:Loss w.r.t Activation: $$\\frac{\\partial L}{\\partial a} = \\frac{a - y}{a(1 - a)}$$Activation w.r.t Linear Sum: $$\\frac{\\partial a}{\\partial z} = a(1 - a)$$Linear Sum w.r.t Weight: $$\\frac{\\partial z}{\\partial w_1} = x_1$$\n",
    "\n",
    "Multiplying them together:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial w_1}\n",
    "=\n",
    "\\frac{\\partial L}{\\partial a}\n",
    "\\cdot\n",
    "\\frac{\\partial a}{\\partial z}\n",
    "\\cdot\n",
    "\\frac{\\partial z}{\\partial w_1}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial w_1}\n",
    "=\n",
    "\\left[ \\frac{a - y}{a(1 - a)} \\right]\n",
    "\\cdot\n",
    "\\left[ a(1 - a) \\right]\n",
    "\\cdot\n",
    "x_1\n",
    "$$\n",
    "\n",
    "\n",
    "The Final Result:$$\\frac{\\partial L}{\\partial w_1} = (a - y)x_1$$ where a is prediction and y is true label .\n",
    "\n",
    "3. Vectorization in NumPySince NumPy 1D arrays are flexible, the gradient for the entire weight vector across $n$ samples can be calculated in two ways depending on the position of the error vector.\n",
    "\n",
    "  Option A:  $X^T$ First (Column Vector Style)If we treat the error as a column, we multiply the transposed feature matrix by the error:$$\\frac{\\partial L}{\\partial w} = \\frac{1}{n} X^T (YPredic - Y)$$Dimensions: $(d \\times n) \\cdot (n \\times 1) = (d \\times 1)$ where d is features and n is samples\n",
    "  \n",
    "  Option B: Error First (Row Vector Style)If we treat the error as a row, we multiply the error by the feature matrix:$$\\frac{\\partial L}{\\partial w} = \\frac{1}{n} (YPredic - Y) X$$Dimensions: $(1 \\times n) \\cdot (n \\times d) = (1 \\times d)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E8bF-iJrcpj4"
   },
   "source": [
    "The Averaging Logic in Vectorized BackpropagationWhen we compute $\\frac{1}{n} X^T (A - Y)$, we are calculating the Mean Gradient for each feature. Here is the expansion for $d$ features and $n$ samples:1. The Vectorized Equation$$\\frac{\\partial J}{\\partial w} = \\frac{1}{n} \\underbrace{\\begin{bmatrix} x_{11} & x_{21} & \\cdots & x_{n1} \\\\ x_{12} & x_{22} & \\cdots & x_{n2} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ x_{1d} & x_{2d} & \\cdots & x_{nd} \\end{bmatrix}}_{X^T (d \\times n)} \\underbrace{\\begin{bmatrix} e_1 \\\\ e_2 \\\\ \\vdots \\\\ e_n \\end{bmatrix}}_{Error (n \\times 1)}$$\n",
    "\n",
    "(Where $e_i = a^{(i)} - y^{(i)}$ is the error for the $i$-th sample)2. The Summation (Feature-wise)After multiplying, the resulting vector contains the sum of (Feature $\\times$ Error) for every sample, divided by $n$:$$\\frac{\\partial J}{\\partial w} = \\begin{bmatrix} \\frac{1}{n} \\left( x_{11}e_1 + x_{21}e_2 + \\cdots + x_{n1}e_n \\right) \\\\ \\frac{1}{n} \\left( x_{12}e_1 + x_{22}e_2 + \\cdots + x_{n2}e_n \\right) \\\\ \\vdots \\\\ \\frac{1}{n} \\left( x_{1d}e_1 + x_{2d}e_2 + \\cdots + x_{nd}e_n \\right) \\end{bmatrix}$$\n",
    "\n",
    "\n",
    "3. Why we divide by $n$ By expanding the first entry (for Feature 1), we see it is literally the average \"blame\" assigned to Weight 1:$$\\frac{\\partial J}{\\partial w_1} = \\frac{\\text{Feature}_1 \\text{ at Sample}_1 \\cdot \\text{Error}_1 + \\dots + \\text{Feature}_1 \\text{ at Sample}_n \\cdot \\text{Error}_n}{n}$$The Logic:If the result is high: This specific feature consistently correlates with our prediction being wrong. Its weight needs a large adjustment.If the result is near zero: On average, this feature doesn't explain the error across the dataset. Its weight stays mostly the same.The Division: Dividing by $n$ ensures that our \"adjustment signal\" doesn't grow simply because we added more data. It keeps the learning rate ($\\alpha$) stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 394,
     "status": "ok",
     "timestamp": 1767369565809,
     "user": {
      "displayName": "Rajdip Sinha",
      "userId": "10072767020088473354"
     },
     "user_tz": -330
    },
    "id": "nWaCFLQV9gBj"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 438,
     "status": "ok",
     "timestamp": 1767369564037,
     "user": {
      "displayName": "Rajdip Sinha",
      "userId": "10072767020088473354"
     },
     "user_tz": -330
    },
    "id": "zO95s_uS_uhz"
   },
   "outputs": [],
   "source": [
    "def sigmoid(input):\n",
    "  return 1.0/(1+np.exp(-1.0*input))\n",
    "\n",
    "  # input will only come as numpy array so no need to make it in numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FSy8xUkbb51f"
   },
   "source": [
    "#                                             **CREATING THE NEURONS OR NODE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 150,
     "status": "ok",
     "timestamp": 1767373912574,
     "user": {
      "displayName": "Rajdip Sinha",
      "userId": "10072767020088473354"
     },
     "user_tz": -330
    },
    "id": "j_sewVuJ7BF8"
   },
   "outputs": [],
   "source": [
    "class Node:\n",
    "  def __init__(self,w,b):\n",
    "\n",
    "# weight = no. of features [w1,w2]\n",
    "    self.weight = w\n",
    "    self.bias = b\n",
    "\n",
    "\n",
    "  def output(self,x):\n",
    "    #  x is feature matrix [[x1,x2][x3,x4]]\n",
    "    o = np.dot(self.weight,x.T) + self.bias  # since we are using numpy we can directly add the bias\n",
    "    return sigmoid(o)\n",
    "  \"\"\"Numpy 1D array isn't either a row or col vector .They behave as row vector if multiplied to a matrix from left ,\n",
    "  otherwise from right , they behave as column vector\"\"\"\n",
    "\n",
    "  def prediction(self,x_test):\n",
    "    return np.dot(x_test,self.weight)+self.bias\n",
    "    # Since w is on left , hence it's treated as col vector to obtain w1*x11 + w2 * x12+...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vYUU55tvdd_q"
   },
   "source": [
    "# **CREATING BINARY CROSS ENTROPY AND ACCURACY MEASURING FUNCTIONS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 340,
     "status": "ok",
     "timestamp": 1767369569704,
     "user": {
      "displayName": "Rajdip Sinha",
      "userId": "10072767020088473354"
     },
     "user_tz": -330
    },
    "id": "ee22m8jvEcMP"
   },
   "outputs": [],
   "source": [
    "# logloss for binary prediction called binary cross entropy\n",
    "def logloss(y_true,y_predic):\n",
    "  eps = 1e-15\n",
    "  # Convert all 0's to 1e-15 and all 1's to 1 - 1e-15 otherwise it will made log function to diverge\n",
    "  y_predic_new = np.array([eps if i == 0 else ((1-eps) if i == 1 else i)for i in y_predic])\n",
    "  return -np.mean((y_true * np.log(y_predic_new)) + (1-y_true)*np.log(1-y_predic_new ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 328,
     "status": "ok",
     "timestamp": 1767371928711,
     "user": {
      "displayName": "Rajdip Sinha",
      "userId": "10072767020088473354"
     },
     "user_tz": -330
    },
    "id": "4mVVccPpZxhm"
   },
   "outputs": [],
   "source": [
    "def accuracy_measurement(y_pred,y_true):\n",
    "  y_pred = np.array([1 if i>0.5 else 0 for i in y_pred])\n",
    "  c = 0\n",
    "  for i in range(len(y_true)):\n",
    "    if y_pred[i] == y_true[i]:\n",
    "      c+=1\n",
    "\n",
    "  return c/len(y_true)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QRU_yAlZdqy0"
   },
   "source": [
    "# **CREATING THE GRADIENT DESCRENT ALGORITHM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 346,
     "status": "ok",
     "timestamp": 1767371998953,
     "user": {
      "displayName": "Rajdip Sinha",
      "userId": "10072767020088473354"
     },
     "user_tz": -330
    },
    "id": "V-N1AGF6BXNB"
   },
   "outputs": [],
   "source": [
    "def gradient_descent(x,y_true,epochs,thres,rate):\n",
    "\n",
    "  w = np.ones(x.shape[1])\n",
    "  b = 0.0\n",
    "  n = len(y_true)\n",
    "# The derivative of logloss gives that w = w - (1/n)*(ypred-y)*x and b = b - mean(ypred-y)\n",
    "\n",
    "  for i in range(epochs):\n",
    "\n",
    "    n1 = Node(w,b)\n",
    "    loss = logloss(y_true,n1.output(x))\n",
    "    accuracy = accuracy_measurement(n1.output(x),y_true)\n",
    "    print(f\"Epoch {i+1}   Loss {loss}    Accuracy {accuracy} \")\n",
    "\n",
    "    if loss < thres:\n",
    "      return w,b\n",
    "\n",
    "    w = w - rate * (1/n) * np.dot((n1.output(x)-y_true) , x)\n",
    "    b = b - rate * np.mean(n1.output(x)-y_true)\n",
    "\n",
    "  return w,b # Return w and b even if threshold is not met"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H7nTwKk0qKhi"
   },
   "source": [
    "## **For two categorical variable use crosstab, or one categorical and continous variable , use groupby , and for Plotting Bar, just write .plot(kind='bar') at the end .**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 2444,
     "status": "ok",
     "timestamp": 1767373530833,
     "user": {
      "displayName": "Rajdip Sinha",
      "userId": "10072767020088473354"
     },
     "user_tz": -330
    },
    "id": "Ck1bFv-aQaXP",
    "outputId": "dd2be979-4dcc-48b9-bfd9-7b879a48828c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>affordibility</th>\n",
       "      <th>bought_insurance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.090909</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.159091</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.659091</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.772727</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.636364</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        age  affordibility  bought_insurance\n",
       "0  0.090909              1                 0\n",
       "1  0.159091              0                 0\n",
       "2  0.659091              1                 1\n",
       "3  0.772727              0                 0\n",
       "4  0.636364              1                 1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_csv(\"insurance_data.csv\")\n",
    "\n",
    "\n",
    "\n",
    "df.groupby('bought_insurance')['age'].mean().plot(kind='bar',ylabel = 'Age')\n",
    "\n",
    "pd.crosstab(df['bought_insurance'],df['affordibility']).plot(kind='bar')\n",
    "\n",
    "# Scaling\n",
    "\n",
    "df['age'] = (df['age'] - df['age'].min())/(df['age'].max()-df['age'].min())\n",
    "df[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n3ptmtMrrBFs"
   },
   "source": [
    "# *From the above , we can conclude that People with higher age and more affordability generally brough the Insurance.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 290,
     "status": "ok",
     "timestamp": 1767371872802,
     "user": {
      "displayName": "Rajdip Sinha",
      "userId": "10072767020088473354"
     },
     "user_tz": -330
    },
    "id": "baGpqFzDUhJb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bought_insurance\n",
       "0    0.25487\n",
       "1    0.74026\n",
       "Name: age, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = df.iloc[:,:-1].to_numpy()\n",
    "target = df['bought_insurance'].to_numpy()\n",
    "df.groupby('bought_insurance')['age'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 349,
     "status": "ok",
     "timestamp": 1767372438266,
     "user": {
      "displayName": "Rajdip Sinha",
      "userId": "10072767020088473354"
     },
     "user_tz": -330
    },
    "id": "1_kCIFQMUvJt",
    "outputId": "c4c8647e-6b7b-4942-f271-d7e4fcec9c26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1   Loss 0.6549069039182462    Accuracy 0.5 \n",
      "Epoch 2   Loss 1.0904320344475484    Accuracy 0.5 \n",
      "Epoch 3   Loss 2.1390804145242006    Accuracy 0.5 \n",
      "Epoch 4   Loss 0.4959657836382701    Accuracy 0.8 \n",
      "Epoch 5   Loss 0.7573105766567233    Accuracy 0.65 \n",
      "Epoch 6   Loss 0.7166273137933997    Accuracy 0.55 \n",
      "Epoch 7   Loss 1.1534619671461193    Accuracy 0.55 \n",
      "Epoch 8   Loss 0.47105481672109306    Accuracy 0.8 \n",
      "Epoch 9   Loss 0.5301424043239893    Accuracy 0.75 \n",
      "Epoch 10   Loss 0.4242373640069402    Accuracy 0.95 \n",
      "Epoch 11   Loss 0.4360334390880757    Accuracy 0.8 \n",
      "Epoch 12   Loss 0.3868436771885696    Accuracy 0.95 \n",
      "Epoch 13   Loss 0.38496030736975534    Accuracy 0.85 \n",
      "Epoch 14   Loss 0.3683940457777431    Accuracy 0.95 \n",
      "Epoch 15   Loss 0.36600180096771057    Accuracy 0.85 \n",
      "Epoch 16   Loss 0.3605076013224359    Accuracy 0.9 \n",
      "Epoch 17   Loss 0.3591285863115302    Accuracy 0.85 \n",
      "Epoch 18   Loss 0.35706879270357916    Accuracy 0.9 \n",
      "Epoch 19   Loss 0.35634120806939734    Accuracy 0.85 \n",
      "Epoch 20   Loss 0.3554550638013604    Accuracy 0.9 \n",
      "Epoch 21   Loss 0.355059381926446    Accuracy 0.85 \n",
      "Epoch 22   Loss 0.3546270873785692    Accuracy 0.9 \n",
      "Epoch 23   Loss 0.3543983227087079    Accuracy 0.85 \n",
      "Epoch 24   Loss 0.3541633500467095    Accuracy 0.9 \n",
      "Epoch 25   Loss 0.3540224145960668    Accuracy 0.85 \n",
      "Epoch 26   Loss 0.353882872743544    Accuracy 0.9 \n",
      "Epoch 27   Loss 0.3537911258945803    Accuracy 0.85 \n",
      "Epoch 28   Loss 0.3537023219287291    Accuracy 0.9 \n",
      "Epoch 29   Loss 0.3536399213402473    Accuracy 0.85 \n",
      "Epoch 30   Loss 0.35358038677448456    Accuracy 0.9 \n",
      "Epoch 31   Loss 0.35353651408869646    Accuracy 0.85 \n",
      "Epoch 32   Loss 0.3534950453784542    Accuracy 0.9 \n",
      "Epoch 33   Loss 0.35346343340164554    Accuracy 0.9 \n",
      "Epoch 34   Loss 0.35343373294553265    Accuracy 0.9 \n",
      "Epoch 35   Loss 0.3534105411593195    Accuracy 0.9 \n",
      "Epoch 36   Loss 0.3533888324011222    Accuracy 0.9 \n",
      "Epoch 37   Loss 0.3533715894876028    Accuracy 0.9 \n",
      "Epoch 38   Loss 0.3533554810167725    Accuracy 0.9 \n",
      "Epoch 39   Loss 0.3533425311166232    Accuracy 0.9 \n",
      "Epoch 40   Loss 0.3533304401481637    Accuracy 0.9 \n",
      "Epoch 41   Loss 0.35332063751300613    Accuracy 0.9 \n",
      "Epoch 42   Loss 0.3533114792534008    Accuracy 0.9 \n",
      "Epoch 43   Loss 0.3533040111210111    Accuracy 0.9 \n",
      "Epoch 44   Loss 0.3532970217861814    Accuracy 0.9 \n",
      "Epoch 45   Loss 0.3532913006044177    Accuracy 0.9 \n",
      "Epoch 46   Loss 0.35328593128954106    Accuracy 0.9 \n",
      "Epoch 47   Loss 0.35328152623165265    Accuracy 0.9 \n",
      "Epoch 48   Loss 0.3532773762943876    Accuracy 0.9 \n",
      "Epoch 49   Loss 0.3532739680658051    Accuracy 0.9 \n",
      "Epoch 50   Loss 0.35327074162722494    Accuracy 0.9 \n",
      "Epoch 51   Loss 0.35326809168125634    Accuracy 0.9 \n",
      "Epoch 52   Loss 0.3532655682660592    Accuracy 0.9 \n",
      "Epoch 53   Loss 0.3532634972866312    Accuracy 0.9 \n",
      "Epoch 54   Loss 0.3532615114645036    Accuracy 0.9 \n",
      "Epoch 55   Loss 0.35325988401564296    Accuracy 0.9 \n",
      "Epoch 56   Loss 0.35325831097699384    Accuracy 0.9 \n",
      "Epoch 57   Loss 0.3532570243761131    Accuracy 0.9 \n",
      "Epoch 58   Loss 0.35325576953821713    Accuracy 0.9 \n",
      "Epoch 59   Loss 0.353254745691774    Accuracy 0.9 \n",
      "Epoch 60   Loss 0.35325373711016456    Accuracy 0.9 \n",
      "Epoch 61   Loss 0.35325291646676116    Accuracy 0.9 \n",
      "Epoch 62   Loss 0.35325209923817197    Accuracy 0.9 \n",
      "Epoch 63   Loss 0.3532514362778628    Accuracy 0.9 \n",
      "Epoch 64   Loss 0.35325076837944164    Accuracy 0.9 \n",
      "Epoch 65   Loss 0.3532502282305117    Accuracy 0.9 \n",
      "Epoch 66   Loss 0.3532496774077173    Accuracy 0.9 \n",
      "Epoch 67   Loss 0.35324923329833374    Accuracy 0.9 \n",
      "Epoch 68   Loss 0.3532487747259357    Accuracy 0.9 \n",
      "Epoch 69   Loss 0.3532484060582457    Accuracy 0.9 \n",
      "Epoch 70   Loss 0.3532480205755677    Accuracy 0.9 \n",
      "Epoch 71   Loss 0.353247711466313    Accuracy 0.9 \n",
      "Epoch 72   Loss 0.35324738424153146    Accuracy 0.9 \n",
      "Epoch 73   Loss 0.353247122413738    Accuracy 0.9 \n",
      "Epoch 74   Loss 0.35324684193024836    Accuracy 0.9 \n",
      "Epoch 75   Loss 0.35324661786996603    Accuracy 0.9 \n",
      "Epoch 76   Loss 0.3532463751554076    Accuracy 0.9 \n",
      "Epoch 77   Loss 0.3532461814690656    Accuracy 0.9 \n",
      "Epoch 78   Loss 0.35324596950772913    Accuracy 0.9 \n",
      "Epoch 79   Loss 0.35324580043165316    Accuracy 0.9 \n",
      "Epoch 80   Loss 0.35324561371582236    Accuracy 0.9 \n",
      "Epoch 81   Loss 0.3532454647413649    Accuracy 0.9 \n",
      "Epoch 82   Loss 0.3532452989281432    Accuracy 0.9 \n",
      "Epoch 83   Loss 0.35324516651479343    Accuracy 0.9 \n",
      "Epoch 84   Loss 0.35324501816316783    Accuracy 0.9 \n",
      "Epoch 85   Loss 0.3532448995186978    Accuracy 0.9 \n",
      "Epoch 86   Loss 0.3532447658877415    Accuracy 0.9 \n",
      "Epoch 87   Loss 0.35324465879950095    Accuracy 0.9 \n",
      "Epoch 88   Loss 0.35324453769323094    Accuracy 0.9 \n",
      "Epoch 89   Loss 0.3532444403985089    Accuracy 0.9 \n",
      "Epoch 90   Loss 0.3532443300464029    Accuracy 0.9 \n",
      "Epoch 91   Loss 0.3532442411326671    Accuracy 0.9 \n",
      "Epoch 92   Loss 0.35324414009747196    Accuracy 0.9 \n",
      "Epoch 93   Loss 0.3532440584254825    Accuracy 0.9 \n",
      "Epoch 94   Loss 0.35324396553194237    Accuracy 0.9 \n",
      "Epoch 95   Loss 0.35324389017639996    Accuracy 0.9 \n",
      "Epoch 96   Loss 0.35324380445604164    Accuracy 0.9 \n",
      "Epoch 97   Loss 0.3532437346596908    Accuracy 0.9 \n",
      "Epoch 98   Loss 0.3532436553079544    Accuracy 0.9 \n",
      "Epoch 99   Loss 0.353243590446028    Accuracy 0.9 \n",
      "Epoch 100   Loss 0.3532435167889015    Accuracy 0.9 \n",
      "Epoch 101   Loss 0.35324345634151993    Accuracy 0.9 \n",
      "Epoch 102   Loss 0.35324338780950437    Accuracy 0.9 \n",
      "Epoch 103   Loss 0.35324333134020414    Accuracy 0.9 \n",
      "Epoch 104   Loss 0.35324326744794143    Accuracy 0.9 \n",
      "Epoch 105   Loss 0.35324321458693364    Accuracy 0.9 \n",
      "Epoch 106   Loss 0.3532431549172158    Accuracy 0.9 \n",
      "Epoch 107   Loss 0.3532431053482992    Accuracy 0.9 \n",
      "Epoch 108   Loss 0.35324304953947666    Accuracy 0.9 \n",
      "Epoch 109   Loss 0.3532430029897792    Accuracy 0.9 \n",
      "Epoch 110   Loss 0.35324295072580836    Accuracy 0.9 \n",
      "Epoch 111   Loss 0.3532429069577215    Accuracy 0.9 \n",
      "Epoch 112   Loss 0.35324285796026833    Accuracy 0.9 \n",
      "Epoch 113   Loss 0.353242816765086    Accuracy 0.9 \n",
      "Epoch 114   Loss 0.3532427707872342    Accuracy 0.9 \n",
      "Epoch 115   Loss 0.35324273198011635    Accuracy 0.9 \n",
      "Epoch 116   Loss 0.35324268880132986    Accuracy 0.9 \n",
      "Epoch 117   Loss 0.35324265221730017    Accuracy 0.9 \n",
      "Epoch 118   Loss 0.35324261163937154    Accuracy 0.9 \n",
      "Epoch 119   Loss 0.3532425771301239    Accuracy 0.9 \n",
      "Epoch 120   Loss 0.3532425389738968    Accuracy 0.9 \n",
      "Epoch 121   Loss 0.3532425064052332    Accuracy 0.9 \n",
      "Epoch 122   Loss 0.35324247050793606    Accuracy 0.9 \n",
      "Epoch 123   Loss 0.353242439757703    Accuracy 0.9 \n",
      "Epoch 124   Loss 0.3532424059707673    Accuracy 0.9 \n",
      "Epoch 125   Loss 0.35324237692718297    Accuracy 0.9 \n",
      "Epoch 126   Loss 0.35324234511444524    Accuracy 0.9 \n",
      "Epoch 127   Loss 0.3532423176747363    Accuracy 0.9 \n",
      "Epoch 128   Loss 0.3532422877109481    Accuracy 0.9 \n",
      "Epoch 129   Loss 0.3532422617802304    Accuracy 0.9 \n",
      "Epoch 130   Loss 0.3532422335498132    Accuracy 0.9 \n",
      "Epoch 131   Loss 0.3532422090401679    Accuracy 0.9 \n",
      "Epoch 132   Loss 0.3532421824361657    Accuracy 0.9 \n",
      "Epoch 133   Loss 0.3532421592658707    Accuracy 0.9 \n",
      "Epoch 134   Loss 0.3532421341890626    Accuracy 0.9 \n",
      "Epoch 135   Loss 0.35324211228194813    Accuracy 0.9 \n",
      "Epoch 136   Loss 0.3532420886400908    Accuracy 0.9 \n",
      "Epoch 137   Loss 0.3532420679249947    Accuracy 0.9 \n",
      "Epoch 138   Loss 0.35324204563217065    Accuracy 0.9 \n",
      "Epoch 139   Loss 0.353242026042475    Accuracy 0.9 \n",
      "Epoch 140   Loss 0.3532420050185281    Accuracy 0.9 \n",
      "Epoch 141   Loss 0.35324198649176075    Accuracy 0.9 \n",
      "Epoch 142   Loss 0.35324196666180363    Accuracy 0.9 \n",
      "Epoch 143   Loss 0.35324194913929435    Accuracy 0.9 \n",
      "Epoch 144   Loss 0.35324193043327534    Accuracy 0.9 \n",
      "Epoch 145   Loss 0.3532419138598546    Accuracy 0.9 \n",
      "Epoch 146   Loss 0.35324189621217517    Accuracy 0.9 \n",
      "Epoch 147   Loss 0.35324188053591027    Accuracy 0.9 \n",
      "Epoch 148   Loss 0.3532418638850847    Accuracy 0.9 \n",
      "Epoch 149   Loss 0.35324184905704553    Accuracy 0.9 \n",
      "Epoch 150   Loss 0.3532418333453957    Accuracy 0.9 \n",
      "Epoch 151   Loss 0.35324181931944554    Accuracy 0.9 \n",
      "Epoch 152   Loss 0.3532418044928269    Accuracy 0.9 \n",
      "Epoch 153   Loss 0.3532417912254354    Accuracy 0.9 \n",
      "Epoch 154   Loss 0.35324177723298883    Accuracy 0.9 \n",
      "Epoch 155   Loss 0.35324176468306207    Accuracy 0.9 \n",
      "Epoch 156   Loss 0.35324175147698844    Accuracy 0.9 \n",
      "Epoch 157   Loss 0.3532417396057149    Accuracy 0.9 \n",
      "Epoch 158   Loss 0.35324172714106966    Accuracy 0.9 \n",
      "Epoch 159   Loss 0.3532417159117793    Accuracy 0.9 \n",
      "Epoch 160   Loss 0.35324170414628453    Accuracy 0.9 \n",
      "Epoch 161   Loss 0.35324169352431983    Accuracy 0.9 \n",
      "Epoch 162   Loss 0.35324168241819115    Accuracy 0.9 \n",
      "Epoch 163   Loss 0.3532416723707874    Accuracy 0.9 \n",
      "Epoch 164   Loss 0.3532416618865748    Accuracy 0.9 \n",
      "Epoch 165   Loss 0.3532416523827505    Accuracy 0.9 \n",
      "Epoch 166   Loss 0.353241642485191    Accuracy 0.9 \n",
      "Epoch 167   Loss 0.3532416334956451    Accuracy 0.9 \n",
      "Epoch 168   Loss 0.35324162415152693    Accuracy 0.9 \n",
      "Epoch 169   Loss 0.3532416156485437    Accuracy 0.9 \n",
      "Epoch 170   Loss 0.35324160682657957    Accuracy 0.9 \n",
      "Epoch 171   Loss 0.3532415987839395    Accuracy 0.9 \n",
      "Epoch 172   Loss 0.35324159045464937    Accuracy 0.9 \n",
      "Epoch 173   Loss 0.3532415828475461    Accuracy 0.9 \n",
      "Epoch 174   Loss 0.35324157498314757    Accuracy 0.9 \n",
      "Epoch 175   Loss 0.35324156778810994    Accuracy 0.9 \n",
      "Epoch 176   Loss 0.3532415603624161    Accuracy 0.9 \n",
      "Epoch 177   Loss 0.35324155355723513    Accuracy 0.9 \n",
      "Epoch 178   Loss 0.3532415465455594    Accuracy 0.9 \n",
      "Epoch 179   Loss 0.3532415401092191    Accuracy 0.9 \n",
      "Epoch 180   Loss 0.35324153348828635    Accuracy 0.9 \n",
      "Epoch 181   Loss 0.35324152740089904    Accuracy 0.9 \n",
      "Epoch 182   Loss 0.35324152114876217    Accuracy 0.9 \n",
      "Epoch 183   Loss 0.3532415153915079    Accuracy 0.9 \n",
      "Epoch 184   Loss 0.3532415094874694    Accuracy 0.9 \n",
      "Epoch 185   Loss 0.3532415040425379    Accuracy 0.9 \n",
      "Epoch 186   Loss 0.35324149846707736    Accuracy 0.9 \n",
      "Epoch 187   Loss 0.353241493317614    Accuracy 0.9 \n",
      "Epoch 188   Loss 0.35324148805231903    Accuracy 0.9 \n",
      "Epoch 189   Loss 0.3532414831823734    Accuracy 0.9 \n",
      "Epoch 190   Loss 0.3532414782098753    Accuracy 0.9 \n",
      "Epoch 191   Loss 0.3532414736043531    Accuracy 0.9 \n",
      "Epoch 192   Loss 0.3532414689082666    Accuracy 0.9 \n",
      "Epoch 193   Loss 0.3532414645528833    Accuracy 0.9 \n",
      "Epoch 194   Loss 0.3532414601177497    Accuracy 0.9 \n",
      "Epoch 195   Loss 0.3532414559989878    Accuracy 0.9 \n",
      "Epoch 196   Loss 0.35324145181022126    Accuracy 0.9 \n",
      "Epoch 197   Loss 0.35324144791528905    Accuracy 0.9 \n",
      "Epoch 198   Loss 0.353241443959127    Accuracy 0.9 \n",
      "Epoch 199   Loss 0.3532414402759198    Accuracy 0.9 \n",
      "Epoch 200   Loss 0.3532414365393751    Accuracy 0.9 \n",
      "Epoch 201   Loss 0.35324143305643874    Accuracy 0.9 \n",
      "Epoch 202   Loss 0.353241429527256    Accuracy 0.9 \n",
      "Epoch 203   Loss 0.3532414262337517    Accuracy 0.9 \n",
      "Epoch 204   Loss 0.35324142290036487    Accuracy 0.9 \n",
      "Epoch 205   Loss 0.35324141978603707    Accuracy 0.9 \n",
      "Epoch 206   Loss 0.3532414166375303    Accuracy 0.9 \n",
      "Epoch 207   Loss 0.353241413692675    Accuracy 0.9 \n",
      "Epoch 208   Loss 0.35324141071874565    Accuracy 0.9 \n",
      "Epoch 209   Loss 0.353241407934181    Accuracy 0.9 \n",
      "Epoch 210   Loss 0.3532414051251047    Accuracy 0.9 \n",
      "Epoch 211   Loss 0.3532414024921436    Accuracy 0.9 \n",
      "Epoch 212   Loss 0.35324139983874164    Accuracy 0.9 \n",
      "Epoch 213   Loss 0.3532413973491645    Accuracy 0.9 \n",
      "Epoch 214   Loss 0.3532413948427725    Accuracy 0.9 \n",
      "Epoch 215   Loss 0.35324139248880304    Accuracy 0.9 \n",
      "Epoch 216   Loss 0.3532413901212422    Accuracy 0.9 \n",
      "Epoch 217   Loss 0.3532413878955232    Accuracy 0.9 \n",
      "Epoch 218   Loss 0.3532413856590725    Accuracy 0.9 \n",
      "Epoch 219   Loss 0.3532413835546437    Accuracy 0.9 \n",
      "Epoch 220   Loss 0.3532413814420141    Accuracy 0.9 \n",
      "Epoch 221   Loss 0.35324137945229095    Accuracy 0.9 \n",
      "Epoch 222   Loss 0.3532413774566011    Accuracy 0.9 \n",
      "Epoch 223   Loss 0.35324137557535434    Accuracy 0.9 \n",
      "Epoch 224   Loss 0.35324137369010744    Accuracy 0.9 \n",
      "Epoch 225   Loss 0.35324137191144456    Accuracy 0.9 \n",
      "Epoch 226   Loss 0.35324137013050677    Accuracy 0.9 \n",
      "Epoch 227   Loss 0.3532413684488534    Accuracy 0.9 \n",
      "Epoch 228   Loss 0.35324136676643336    Accuracy 0.9 \n",
      "Epoch 229   Loss 0.3532413651765166    Accuracy 0.9 \n",
      "Epoch 230   Loss 0.35324136358714625    Accuracy 0.9 \n",
      "Epoch 231   Loss 0.35324136208397827    Accuracy 0.9 \n",
      "Epoch 232   Loss 0.35324136058249433    Accuracy 0.9 \n",
      "Epoch 233   Loss 0.35324135916135735    Accuracy 0.9 \n",
      "Epoch 234   Loss 0.3532413577428848    Accuracy 0.9 \n",
      "Epoch 235   Loss 0.3532413563993163    Accuracy 0.9 \n",
      "Epoch 236   Loss 0.35324135505925136    Accuracy 0.9 \n",
      "Epoch 237   Loss 0.35324135378903043    Accuracy 0.9 \n",
      "Epoch 238   Loss 0.3532413525230263    Accuracy 0.9 \n",
      "Epoch 239   Loss 0.35324135132216067    Accuracy 0.9 \n",
      "Epoch 240   Loss 0.3532413501261122    Accuracy 0.9 \n",
      "Epoch 241   Loss 0.353241348990826    Accuracy 0.9 \n",
      "Epoch 242   Loss 0.35324134786085665    Accuracy 0.9 \n",
      "Epoch 243   Loss 0.35324134678757874    Accuracy 0.9 \n",
      "Epoch 244   Loss 0.3532413457200279    Accuracy 0.9 \n",
      "Epoch 245   Loss 0.35324134470538054    Accuracy 0.9 \n",
      "Epoch 246   Loss 0.35324134369679105    Accuracy 0.9 \n",
      "Epoch 247   Loss 0.3532413427375801    Accuracy 0.9 \n",
      "Epoch 248   Loss 0.35324134178468697    Accuracy 0.9 \n",
      "Epoch 249   Loss 0.3532413408778914    Accuracy 0.9 \n",
      "Epoch 250   Loss 0.35324133997761126    Accuracy 0.9 \n",
      "Epoch 251   Loss 0.3532413391203741    Accuracy 0.9 \n",
      "Epoch 252   Loss 0.35324133826979476    Accuracy 0.9 \n",
      "Epoch 253   Loss 0.3532413374594142    Accuracy 0.9 \n",
      "Epoch 254   Loss 0.3532413366557854    Accuracy 0.9 \n",
      "Epoch 255   Loss 0.35324133588970624    Accuracy 0.9 \n",
      "Epoch 256   Loss 0.3532413351304305    Accuracy 0.9 \n",
      "Epoch 257   Loss 0.3532413344062365    Accuracy 0.9 \n",
      "Epoch 258   Loss 0.3532413336888604    Accuracy 0.9 \n",
      "Epoch 259   Loss 0.35324133300426674    Accuracy 0.9 \n",
      "Epoch 260   Loss 0.3532413323264728    Accuracy 0.9 \n",
      "Epoch 261   Loss 0.353241331679319    Accuracy 0.9 \n",
      "Epoch 262   Loss 0.3532413310389188    Accuracy 0.9 \n",
      "Epoch 263   Loss 0.35324133042716127    Accuracy 0.9 \n",
      "Epoch 264   Loss 0.35324132982208767    Accuracy 0.9 \n",
      "Epoch 265   Loss 0.3532413292437946    Accuracy 0.9 \n",
      "Epoch 266   Loss 0.3532413286720946    Accuracy 0.9 \n",
      "Epoch 267   Loss 0.3532413281254391    Accuracy 0.9 \n",
      "Epoch 268   Loss 0.35324132758526855    Accuracy 0.9 \n",
      "Epoch 269   Loss 0.3532413270685232    Accuracy 0.9 \n",
      "Epoch 270   Loss 0.3532413265581399    Accuracy 0.9 \n",
      "Epoch 271   Loss 0.35324132606967124    Accuracy 0.9 \n",
      "Epoch 272   Loss 0.3532413255874295    Accuracy 0.9 \n",
      "Epoch 273   Loss 0.35324132512569306    Accuracy 0.9 \n",
      "Epoch 274   Loss 0.35324132467003855    Accuracy 0.9 \n",
      "Epoch 275   Loss 0.35324132423357396    Accuracy 0.9 \n",
      "Epoch 276   Loss 0.3532413238030382    Accuracy 0.9 \n",
      "Epoch 277   Loss 0.35324132339046466    Accuracy 0.9 \n",
      "Epoch 278   Loss 0.3532413229836607    Accuracy 0.9 \n",
      "Epoch 279   Loss 0.3532413225936727    Accuracy 0.9 \n",
      "Epoch 280   Loss 0.35324132220929017    Accuracy 0.9 \n",
      "Epoch 281   Loss 0.35324132184065343    Accuracy 0.9 \n",
      "Epoch 282   Loss 0.35324132147745463    Accuracy 0.9 \n",
      "Epoch 283   Loss 0.35324132112900203    Accuracy 0.9 \n",
      "Epoch 284   Loss 0.35324132078581744    Accuracy 0.9 \n",
      "Epoch 285   Loss 0.35324132045644563    Accuracy 0.9 \n",
      "Epoch 286   Loss 0.3532413201321709    Accuracy 0.9 \n",
      "Epoch 287   Loss 0.35324131982083656    Accuracy 0.9 \n",
      "Epoch 288   Loss 0.3532413195144282    Accuracy 0.9 \n",
      "Epoch 289   Loss 0.35324131922014484    Accuracy 0.9 \n",
      "Epoch 290   Loss 0.3532413189306171    Accuracy 0.9 \n",
      "Epoch 291   Loss 0.35324131865245245    Accuracy 0.9 \n",
      "Epoch 292   Loss 0.3532413183788739    Accuracy 0.9 \n",
      "Epoch 293   Loss 0.35324131811594617    Accuracy 0.9 \n",
      "Epoch 294   Loss 0.3532413178574371    Accuracy 0.9 \n",
      "Epoch 295   Loss 0.353241317608913    Accuracy 0.9 \n",
      "Epoch 296   Loss 0.35324131736464226    Accuracy 0.9 \n",
      "Epoch 297   Loss 0.35324131712973356    Accuracy 0.9 \n",
      "Epoch 298   Loss 0.3532413168989158    Accuracy 0.9 \n",
      "Epoch 299   Loss 0.35324131667687775    Accuracy 0.9 \n",
      "Epoch 300   Loss 0.35324131645877127    Accuracy 0.9 \n",
      "Epoch 301   Loss 0.3532413162488993    Accuracy 0.9 \n",
      "Epoch 302   Loss 0.3532413160428033    Accuracy 0.9 \n",
      "Epoch 303   Loss 0.35324131584443164    Accuracy 0.9 \n",
      "Epoch 304   Loss 0.3532413156496838    Accuracy 0.9 \n",
      "Epoch 305   Loss 0.3532413154621831    Accuracy 0.9 \n",
      "Epoch 306   Loss 0.35324131527815805    Accuracy 0.9 \n",
      "Epoch 307   Loss 0.35324131510093315    Accuracy 0.9 \n",
      "Epoch 308   Loss 0.35324131492703975    Accuracy 0.9 \n",
      "Epoch 309   Loss 0.3532413147595282    Accuracy 0.9 \n",
      "Epoch 310   Loss 0.35324131459520797    Accuracy 0.9 \n",
      "Epoch 311   Loss 0.35324131443687806    Accuracy 0.9 \n",
      "Epoch 312   Loss 0.35324131428160327    Accuracy 0.9 \n",
      "Epoch 313   Loss 0.3532413141319523    Accuracy 0.9 \n",
      "Epoch 314   Loss 0.3532413139852247    Accuracy 0.9 \n",
      "Epoch 315   Loss 0.3532413138437772    Accuracy 0.9 \n",
      "Epoch 316   Loss 0.3532413137051259    Accuracy 0.9 \n",
      "Epoch 317   Loss 0.3532413135714327    Accuracy 0.9 \n",
      "Epoch 318   Loss 0.3532413134404126    Accuracy 0.9 \n",
      "Epoch 319   Loss 0.353241313314049    Accuracy 0.9 \n",
      "Epoch 320   Loss 0.3532413131902398    Accuracy 0.9 \n",
      "Epoch 321   Loss 0.3532413130708045    Accuracy 0.9 \n",
      "Epoch 322   Loss 0.35324131295380884    Accuracy 0.9 \n",
      "Epoch 323   Loss 0.35324131284092203    Accuracy 0.9 \n",
      "Epoch 324   Loss 0.35324131273036474    Accuracy 0.9 \n",
      "Epoch 325   Loss 0.35324131262366804    Accuracy 0.9 \n",
      "Epoch 326   Loss 0.3532413125191944    Accuracy 0.9 \n",
      "Epoch 327   Loss 0.35324131241834833    Accuracy 0.9 \n",
      "Epoch 328   Loss 0.35324131231962336    Accuracy 0.9 \n",
      "Epoch 329   Loss 0.35324131222430755    Accuracy 0.9 \n",
      "Epoch 330   Loss 0.3532413121310148    Accuracy 0.9 \n",
      "Epoch 331   Loss 0.353241312040926    Accuracy 0.9 \n",
      "Epoch 332   Loss 0.3532413119527662    Accuracy 0.9 \n",
      "Epoch 333   Loss 0.35324131186761826    Accuracy 0.9 \n",
      "Epoch 334   Loss 0.3532413117843087    Accuracy 0.9 \n",
      "Epoch 335   Loss 0.35324131170383083    Accuracy 0.9 \n",
      "Epoch 336   Loss 0.3532413116251045    Accuracy 0.9 \n",
      "Epoch 337   Loss 0.3532413115490407    Accuracy 0.9 \n",
      "Epoch 338   Loss 0.3532413114746454    Accuracy 0.9 \n",
      "Epoch 339   Loss 0.35324131140275367    Accuracy 0.9 \n",
      "Epoch 340   Loss 0.35324131133245085    Accuracy 0.9 \n",
      "Epoch 341   Loss 0.3532413112645026    Accuracy 0.9 \n",
      "Epoch 342   Loss 0.353241311198067    Accuracy 0.9 \n",
      "Epoch 343   Loss 0.35324131113384605    Accuracy 0.9 \n",
      "Epoch 344   Loss 0.3532413110710648    Accuracy 0.9 \n",
      "Epoch 345   Loss 0.35324131101036677    Accuracy 0.9 \n",
      "Epoch 346   Loss 0.3532413109510387    Accuracy 0.9 \n",
      "Epoch 347   Loss 0.3532413108936706    Accuracy 0.9 \n",
      "Epoch 348   Loss 0.3532413108376057    Accuracy 0.9 \n",
      "Epoch 349   Loss 0.35324131078338483    Accuracy 0.9 \n",
      "Epoch 350   Loss 0.35324131073040343    Accuracy 0.9 \n",
      "Epoch 351   Loss 0.3532413106791573    Accuracy 0.9 \n",
      "Epoch 352   Loss 0.3532413106290898    Accuracy 0.9 \n",
      "Epoch 353   Loss 0.3532413105806554    Accuracy 0.9 \n",
      "Epoch 354   Loss 0.3532413105333413    Accuracy 0.9 \n",
      "Epoch 355   Loss 0.35324131048756435    Accuracy 0.9 \n",
      "Epoch 356   Loss 0.3532413104428523    Accuracy 0.9 \n",
      "Epoch 357   Loss 0.35324131039958695    Accuracy 0.9 \n",
      "Epoch 358   Loss 0.3532413103573338    Accuracy 0.9 \n",
      "Epoch 359   Loss 0.3532413103164424    Accuracy 0.9 \n",
      "Epoch 360   Loss 0.35324131027651284    Accuracy 0.9 \n",
      "Epoch 361   Loss 0.35324131023786537    Accuracy 0.9 \n",
      "Epoch 362   Loss 0.3532413102001313    Accuracy 0.9 \n",
      "Epoch 363   Loss 0.3532413101636045    Accuracy 0.9 \n",
      "Epoch 364   Loss 0.3532413101279454    Accuracy 0.9 \n",
      "Epoch 365   Loss 0.353241310093423    Accuracy 0.9 \n",
      "Epoch 366   Loss 0.35324131005972476    Accuracy 0.9 \n",
      "Epoch 367   Loss 0.3532413100270968    Accuracy 0.9 \n",
      "Epoch 368   Loss 0.3532413099952514    Accuracy 0.9 \n",
      "Epoch 369   Loss 0.35324130996441394    Accuracy 0.9 \n",
      "Epoch 370   Loss 0.35324130993431957    Accuracy 0.9 \n",
      "Epoch 371   Loss 0.3532413099051745    Accuracy 0.9 \n",
      "Epoch 372   Loss 0.3532413098767346    Accuracy 0.9 \n",
      "Epoch 373   Loss 0.3532413098491891    Accuracy 0.9 \n",
      "Epoch 374   Loss 0.35324130982231294    Accuracy 0.9 \n",
      "Epoch 375   Loss 0.35324130979627916    Accuracy 0.9 \n",
      "Epoch 376   Loss 0.3532413097708806    Accuracy 0.9 \n",
      "Epoch 377   Loss 0.35324130974627577    Accuracy 0.9 \n",
      "Epoch 378   Loss 0.35324130972227347    Accuracy 0.9 \n",
      "Epoch 379   Loss 0.35324130969901896    Accuracy 0.9 \n",
      "Epoch 380   Loss 0.3532413096763363    Accuracy 0.9 \n",
      "Epoch 381   Loss 0.3532413096543582    Accuracy 0.9 \n",
      "Epoch 382   Loss 0.3532413096329225    Accuracy 0.9 \n",
      "Epoch 383   Loss 0.3532413096121507    Accuracy 0.9 \n",
      "Epoch 384   Loss 0.35324130959189337    Accuracy 0.9 \n",
      "Epoch 385   Loss 0.35324130957226174    Accuracy 0.9 \n",
      "Epoch 386   Loss 0.35324130955311805    Accuracy 0.9 \n",
      "Epoch 387   Loss 0.353241309534564    Accuracy 0.9 \n",
      "Epoch 388   Loss 0.3532413095164726    Accuracy 0.9 \n",
      "Epoch 389   Loss 0.353241309498937    Accuracy 0.9 \n",
      "Epoch 390   Loss 0.3532413094818401    Accuracy 0.9 \n",
      "Epoch 391   Loss 0.35324130946526705    Accuracy 0.9 \n",
      "Epoch 392   Loss 0.35324130944911003    Accuracy 0.9 \n",
      "Epoch 393   Loss 0.35324130943344667    Accuracy 0.9 \n",
      "Epoch 394   Loss 0.3532413094181778    Accuracy 0.9 \n",
      "Epoch 395   Loss 0.3532413094033743    Accuracy 0.9 \n",
      "Epoch 396   Loss 0.35324130938894455    Accuracy 0.9 \n",
      "Epoch 397   Loss 0.3532413093749538    Accuracy 0.9 \n",
      "Epoch 398   Loss 0.35324130936131726    Accuracy 0.9 \n",
      "Epoch 399   Loss 0.3532413093480945    Accuracy 0.9 \n",
      "Epoch 400   Loss 0.3532413093352075    Accuracy 0.9 \n",
      "Epoch 401   Loss 0.3532413093227106    Accuracy 0.9 \n",
      "Epoch 402   Loss 0.35324130931053194    Accuracy 0.9 \n",
      "Epoch 403   Loss 0.3532413092987211    Accuracy 0.9 \n",
      "Epoch 404   Loss 0.3532413092872118    Accuracy 0.9 \n",
      "Epoch 405   Loss 0.35324130927604946    Accuracy 0.9 \n",
      "Epoch 406   Loss 0.35324130926517266    Accuracy 0.9 \n",
      "Epoch 407   Loss 0.35324130925462316    Accuracy 0.9 \n",
      "Epoch 408   Loss 0.35324130924434416    Accuracy 0.9 \n",
      "Epoch 409   Loss 0.3532413092343737    Accuracy 0.9 \n",
      "Epoch 410   Loss 0.35324130922465974    Accuracy 0.9 \n",
      "Epoch 411   Loss 0.3532413092152368    Accuracy 0.9 \n",
      "Epoch 412   Loss 0.3532413092060567    Accuracy 0.9 \n",
      "Epoch 413   Loss 0.353241309197151    Accuracy 0.9 \n",
      "Epoch 414   Loss 0.3532413091884754    Accuracy 0.9 \n",
      "Epoch 415   Loss 0.3532413091800587    Accuracy 0.9 \n",
      "Epoch 416   Loss 0.3532413091718599    Accuracy 0.9 \n",
      "Epoch 417   Loss 0.3532413091639052    Accuracy 0.9 \n",
      "Epoch 418   Loss 0.35324130915615704    Accuracy 0.9 \n",
      "Epoch 419   Loss 0.3532413091486392    Accuracy 0.9 \n",
      "Epoch 420   Loss 0.3532413091413168    Accuracy 0.9 \n",
      "Epoch 421   Loss 0.35324130913421176    Accuracy 0.9 \n",
      "Epoch 422   Loss 0.3532413091272918    Accuracy 0.9 \n",
      "Epoch 423   Loss 0.35324130912057683    Accuracy 0.9 \n",
      "Epoch 424   Loss 0.35324130911403706    Accuracy 0.9 \n",
      "Epoch 425   Loss 0.35324130910769086    Accuracy 0.9 \n",
      "Epoch 426   Loss 0.3532413091015105    Accuracy 0.9 \n",
      "Epoch 427   Loss 0.35324130909551277    Accuracy 0.9 \n",
      "Epoch 428   Loss 0.35324130908967205    Accuracy 0.9 \n",
      "Epoch 429   Loss 0.3532413090840036    Accuracy 0.9 \n",
      "Epoch 430   Loss 0.3532413090784839    Accuracy 0.9 \n",
      "Epoch 431   Loss 0.3532413090731267    Accuracy 0.9 \n",
      "Epoch 432   Loss 0.35324130906791024    Accuracy 0.9 \n",
      "Epoch 433   Loss 0.3532413090628471    Accuracy 0.9 \n",
      "Epoch 434   Loss 0.3532413090579174    Accuracy 0.9 \n",
      "Epoch 435   Loss 0.3532413090531324    Accuracy 0.9 \n",
      "Epoch 436   Loss 0.3532413090484735    Accuracy 0.9 \n",
      "Epoch 437   Loss 0.35324130904395123    Accuracy 0.9 \n",
      "Epoch 438   Loss 0.3532413090395483    Accuracy 0.9 \n",
      "Epoch 439   Loss 0.3532413090352744    Accuracy 0.9 \n",
      "Epoch 440   Loss 0.35324130903111345    Accuracy 0.9 \n",
      "Epoch 441   Loss 0.35324130902707424    Accuracy 0.9 \n",
      "Epoch 442   Loss 0.353241309023142    Accuracy 0.9 \n",
      "Epoch 443   Loss 0.3532413090193246    Accuracy 0.9 \n",
      "Epoch 444   Loss 0.3532413090156083    Accuracy 0.9 \n",
      "Epoch 445   Loss 0.35324130901200046    Accuracy 0.9 \n",
      "Epoch 446   Loss 0.35324130900848844    Accuracy 0.9 \n",
      "Epoch 447   Loss 0.3532413090050788    Accuracy 0.9 \n",
      "Epoch 448   Loss 0.35324130900175976    Accuracy 0.9 \n",
      "Epoch 449   Loss 0.35324130899853723    Accuracy 0.9 \n",
      "Epoch 450   Loss 0.35324130899540057    Accuracy 0.9 \n",
      "Epoch 451   Loss 0.35324130899235506    Accuracy 0.9 \n",
      "Epoch 452   Loss 0.35324130898939077    Accuracy 0.9 \n",
      "Epoch 453   Loss 0.3532413089865125    Accuracy 0.9 \n",
      "Epoch 454   Loss 0.3532413089837111    Accuracy 0.9 \n",
      "Epoch 455   Loss 0.353241308980991    Accuracy 0.9 \n",
      "Epoch 456   Loss 0.3532413089783434    Accuracy 0.9 \n",
      "Epoch 457   Loss 0.35324130897577255    Accuracy 0.9 \n",
      "Epoch 458   Loss 0.35324130897327055    Accuracy 0.9 \n",
      "Epoch 459   Loss 0.35324130897084094    Accuracy 0.9 \n",
      "Epoch 460   Loss 0.3532413089684764    Accuracy 0.9 \n",
      "Epoch 461   Loss 0.35324130896618017    Accuracy 0.9 \n",
      "Epoch 462   Loss 0.35324130896394557    Accuracy 0.9 \n",
      "Epoch 463   Loss 0.3532413089617754    Accuracy 0.9 \n",
      "Epoch 464   Loss 0.35324130895966366    Accuracy 0.9 \n",
      "Epoch 465   Loss 0.35324130895761263    Accuracy 0.9 \n",
      "Epoch 466   Loss 0.35324130895561684    Accuracy 0.9 \n",
      "Epoch 467   Loss 0.3532413089536785    Accuracy 0.9 \n",
      "Epoch 468   Loss 0.3532413089517924    Accuracy 0.9 \n",
      "Epoch 469   Loss 0.3532413089499605    Accuracy 0.9 \n",
      "Epoch 470   Loss 0.35324130894817796    Accuracy 0.9 \n",
      "Epoch 471   Loss 0.35324130894644673    Accuracy 0.9 \n",
      "Epoch 472   Loss 0.35324130894476213    Accuracy 0.9 \n",
      "Epoch 473   Loss 0.353241308943126    Accuracy 0.9 \n",
      "Epoch 474   Loss 0.35324130894153394    Accuracy 0.9 \n",
      "Epoch 475   Loss 0.3532413089399876    Accuracy 0.9 \n",
      "Epoch 476   Loss 0.35324130893848305    Accuracy 0.9 \n",
      "Epoch 477   Loss 0.3532413089370216    Accuracy 0.9 \n",
      "Epoch 478   Loss 0.35324130893559974    Accuracy 0.9 \n",
      "Epoch 479   Loss 0.3532413089342187    Accuracy 0.9 \n",
      "Epoch 480   Loss 0.35324130893287486    Accuracy 0.9 \n",
      "Epoch 481   Loss 0.3532413089315696    Accuracy 0.9 \n",
      "Epoch 482   Loss 0.35324130893029965    Accuracy 0.9 \n",
      "Epoch 483   Loss 0.3532413089290661    Accuracy 0.9 \n",
      "Epoch 484   Loss 0.3532413089278658    Accuracy 0.9 \n",
      "Epoch 485   Loss 0.3532413089267    Accuracy 0.9 \n",
      "Epoch 486   Loss 0.35324130892556577    Accuracy 0.9 \n",
      "Epoch 487   Loss 0.35324130892446404    Accuracy 0.9 \n",
      "Epoch 488   Loss 0.35324130892339206    Accuracy 0.9 \n",
      "Epoch 489   Loss 0.3532413089223508    Accuracy 0.9 \n",
      "Epoch 490   Loss 0.35324130892133765    Accuracy 0.9 \n",
      "Epoch 491   Loss 0.35324130892035355    Accuracy 0.9 \n",
      "Epoch 492   Loss 0.35324130891939626    Accuracy 0.9 \n",
      "Epoch 493   Loss 0.35324130891846617    Accuracy 0.9 \n",
      "Epoch 494   Loss 0.35324130891756134    Accuracy 0.9 \n",
      "Epoch 495   Loss 0.35324130891668243    Accuracy 0.9 \n",
      "Epoch 496   Loss 0.3532413089158274    Accuracy 0.9 \n",
      "Epoch 497   Loss 0.3532413089149967    Accuracy 0.9 \n",
      "Epoch 498   Loss 0.3532413089141886    Accuracy 0.9 \n",
      "Epoch 499   Loss 0.3532413089134035    Accuracy 0.9 \n",
      "Epoch 500   Loss 0.35324130891263983    Accuracy 0.9 \n",
      "Epoch 501   Loss 0.35324130891189787    Accuracy 0.9 \n",
      "Epoch 502   Loss 0.35324130891117617    Accuracy 0.9 \n",
      "Epoch 503   Loss 0.35324130891047484    Accuracy 0.9 \n",
      "Epoch 504   Loss 0.35324130890979283    Accuracy 0.9 \n",
      "Epoch 505   Loss 0.3532413089091301    Accuracy 0.9 \n",
      "Epoch 506   Loss 0.35324130890848543    Accuracy 0.9 \n",
      "Epoch 507   Loss 0.35324130890785915    Accuracy 0.9 \n",
      "Epoch 508   Loss 0.35324130890725003    Accuracy 0.9 \n",
      "Epoch 509   Loss 0.35324130890665806    Accuracy 0.9 \n",
      "Epoch 510   Loss 0.3532413089060823    Accuracy 0.9 \n",
      "Epoch 511   Loss 0.35324130890552297    Accuracy 0.9 \n",
      "Epoch 512   Loss 0.35324130890497885    Accuracy 0.9 \n",
      "Epoch 513   Loss 0.3532413089044502    Accuracy 0.9 \n",
      "Epoch 514   Loss 0.353241308903936    Accuracy 0.9 \n",
      "Epoch 515   Loss 0.35324130890343636    Accuracy 0.9 \n",
      "Epoch 516   Loss 0.3532413089029504    Accuracy 0.9 \n",
      "Epoch 517   Loss 0.3532413089024782    Accuracy 0.9 \n",
      "Epoch 518   Loss 0.3532413089020189    Accuracy 0.9 \n",
      "Epoch 519   Loss 0.35324130890157274    Accuracy 0.9 \n",
      "Epoch 520   Loss 0.35324130890113864    Accuracy 0.9 \n",
      "Epoch 521   Loss 0.35324130890071687    Accuracy 0.9 \n",
      "Epoch 522   Loss 0.3532413089003067    Accuracy 0.9 \n",
      "Epoch 523   Loss 0.3532413088999081    Accuracy 0.9 \n",
      "Epoch 524   Loss 0.3532413088995205    Accuracy 0.9 \n",
      "Epoch 525   Loss 0.3532413088991438    Accuracy 0.9 \n",
      "Epoch 526   Loss 0.3532413088987774    Accuracy 0.9 \n",
      "Epoch 527   Loss 0.35324130889842154    Accuracy 0.9 \n",
      "Epoch 528   Loss 0.35324130889807515    Accuracy 0.9 \n",
      "Epoch 529   Loss 0.35324130889773875    Accuracy 0.9 \n",
      "Epoch 530   Loss 0.3532413088974115    Accuracy 0.9 \n",
      "Epoch 531   Loss 0.35324130889709365    Accuracy 0.9 \n",
      "Epoch 532   Loss 0.35324130889678435    Accuracy 0.9 \n",
      "Epoch 533   Loss 0.35324130889648386    Accuracy 0.9 \n",
      "Epoch 534   Loss 0.35324130889619154    Accuracy 0.9 \n",
      "Epoch 535   Loss 0.35324130889590755    Accuracy 0.9 \n",
      "Epoch 536   Loss 0.35324130889563143    Accuracy 0.9 \n",
      "Epoch 537   Loss 0.353241308895363    Accuracy 0.9 \n",
      "Epoch 538   Loss 0.35324130889510197    Accuracy 0.9 \n",
      "Epoch 539   Loss 0.3532413088948484    Accuracy 0.9 \n",
      "Epoch 540   Loss 0.3532413088946017    Accuracy 0.9 \n",
      "Epoch 541   Loss 0.35324130889436195    Accuracy 0.9 \n",
      "Epoch 542   Loss 0.3532413088941288    Accuracy 0.9 \n",
      "Epoch 543   Loss 0.3532413088939024    Accuracy 0.9 \n",
      "Epoch 544   Loss 0.35324130889368194    Accuracy 0.9 \n",
      "Epoch 545   Loss 0.3532413088934678    Accuracy 0.9 \n",
      "Epoch 546   Loss 0.35324130889325955    Accuracy 0.9 \n",
      "Epoch 547   Loss 0.3532413088930572    Accuracy 0.9 \n",
      "Epoch 548   Loss 0.35324130889286043    Accuracy 0.9 \n",
      "Epoch 549   Loss 0.3532413088926692    Accuracy 0.9 \n",
      "Epoch 550   Loss 0.35324130889248334    Accuracy 0.9 \n",
      "Epoch 551   Loss 0.35324130889230254    Accuracy 0.9 \n",
      "Epoch 552   Loss 0.3532413088921268    Accuracy 0.9 \n",
      "Epoch 553   Loss 0.35324130889195604    Accuracy 0.9 \n",
      "Epoch 554   Loss 0.35324130889178984    Accuracy 0.9 \n",
      "Epoch 555   Loss 0.3532413088916285    Accuracy 0.9 \n",
      "Epoch 556   Loss 0.3532413088914715    Accuracy 0.9 \n",
      "Epoch 557   Loss 0.353241308891319    Accuracy 0.9 \n",
      "Epoch 558   Loss 0.35324130889117056    Accuracy 0.9 \n",
      "Epoch 559   Loss 0.35324130889102634    Accuracy 0.9 \n",
      "Epoch 560   Loss 0.35324130889088623    Accuracy 0.9 \n",
      "Epoch 561   Loss 0.35324130889074995    Accuracy 0.9 \n",
      "Epoch 562   Loss 0.3532413088906175    Accuracy 0.9 \n",
      "Epoch 563   Loss 0.35324130889048866    Accuracy 0.9 \n",
      "Epoch 564   Loss 0.3532413088903634    Accuracy 0.9 \n",
      "Epoch 565   Loss 0.3532413088902417    Accuracy 0.9 \n",
      "Epoch 566   Loss 0.3532413088901234    Accuracy 0.9 \n",
      "Epoch 567   Loss 0.35324130889000843    Accuracy 0.9 \n",
      "Epoch 568   Loss 0.3532413088898966    Accuracy 0.9 \n",
      "Epoch 569   Loss 0.35324130888978794    Accuracy 0.9 \n",
      "Epoch 570   Loss 0.3532413088896822    Accuracy 0.9 \n",
      "Epoch 571   Loss 0.35324130888957933    Accuracy 0.9 \n",
      "Epoch 572   Loss 0.3532413088894795    Accuracy 0.9 \n",
      "Epoch 573   Loss 0.3532413088893824    Accuracy 0.9 \n",
      "Epoch 574   Loss 0.35324130888928806    Accuracy 0.9 \n",
      "Epoch 575   Loss 0.3532413088891963    Accuracy 0.9 \n",
      "Epoch 576   Loss 0.35324130888910715    Accuracy 0.9 \n",
      "Epoch 577   Loss 0.3532413088890204    Accuracy 0.9 \n",
      "Epoch 578   Loss 0.353241308888936    Accuracy 0.9 \n",
      "Epoch 579   Loss 0.3532413088888541    Accuracy 0.9 \n",
      "Epoch 580   Loss 0.3532413088887745    Accuracy 0.9 \n",
      "Epoch 581   Loss 0.35324130888869704    Accuracy 0.9 \n",
      "Epoch 582   Loss 0.35324130888862165    Accuracy 0.9 \n",
      "Epoch 583   Loss 0.3532413088885485    Accuracy 0.9 \n",
      "Epoch 584   Loss 0.35324130888847727    Accuracy 0.9 \n",
      "Epoch 585   Loss 0.3532413088884081    Accuracy 0.9 \n",
      "Epoch 586   Loss 0.3532413088883408    Accuracy 0.9 \n",
      "Epoch 587   Loss 0.3532413088882755    Accuracy 0.9 \n",
      "Epoch 588   Loss 0.3532413088882119    Accuracy 0.9 \n",
      "Epoch 589   Loss 0.3532413088881502    Accuracy 0.9 \n",
      "Epoch 590   Loss 0.35324130888809013    Accuracy 0.9 \n",
      "Epoch 591   Loss 0.3532413088880316    Accuracy 0.9 \n",
      "Epoch 592   Loss 0.35324130888797484    Accuracy 0.9 \n",
      "Epoch 593   Loss 0.3532413088879197    Accuracy 0.9 \n",
      "Epoch 594   Loss 0.3532413088878661    Accuracy 0.9 \n",
      "Epoch 595   Loss 0.35324130888781385    Accuracy 0.9 \n",
      "Epoch 596   Loss 0.35324130888776323    Accuracy 0.9 \n",
      "Epoch 597   Loss 0.35324130888771393    Accuracy 0.9 \n",
      "Epoch 598   Loss 0.35324130888766603    Accuracy 0.9 \n",
      "Epoch 599   Loss 0.3532413088876194    Accuracy 0.9 \n",
      "Epoch 600   Loss 0.3532413088875741    Accuracy 0.9 \n",
      "Epoch 601   Loss 0.35324130888753014    Accuracy 0.9 \n",
      "Epoch 602   Loss 0.3532413088874874    Accuracy 0.9 \n",
      "Epoch 603   Loss 0.35324130888744565    Accuracy 0.9 \n",
      "Epoch 604   Loss 0.3532413088874053    Accuracy 0.9 \n",
      "Epoch 605   Loss 0.3532413088873659    Accuracy 0.9 \n",
      "Epoch 606   Loss 0.35324130888732763    Accuracy 0.9 \n",
      "Epoch 607   Loss 0.35324130888729055    Accuracy 0.9 \n",
      "Epoch 608   Loss 0.35324130888725436    Accuracy 0.9 \n",
      "Epoch 609   Loss 0.35324130888721933    Accuracy 0.9 \n",
      "Epoch 610   Loss 0.3532413088871852    Accuracy 0.9 \n",
      "Epoch 611   Loss 0.353241308887152    Accuracy 0.9 \n",
      "Epoch 612   Loss 0.3532413088871197    Accuracy 0.9 \n",
      "Epoch 613   Loss 0.35324130888708827    Accuracy 0.9 \n",
      "Epoch 614   Loss 0.3532413088870578    Accuracy 0.9 \n",
      "Epoch 615   Loss 0.3532413088870282    Accuracy 0.9 \n",
      "Epoch 616   Loss 0.35324130888699934    Accuracy 0.9 \n",
      "Epoch 617   Loss 0.35324130888697136    Accuracy 0.9 \n",
      "Epoch 618   Loss 0.3532413088869441    Accuracy 0.9 \n",
      "Epoch 619   Loss 0.3532413088869177    Accuracy 0.9 \n",
      "Epoch 620   Loss 0.3532413088868919    Accuracy 0.9 \n",
      "Epoch 621   Loss 0.3532413088868669    Accuracy 0.9 \n",
      "Epoch 622   Loss 0.3532413088868426    Accuracy 0.9 \n",
      "Epoch 623   Loss 0.3532413088868189    Accuracy 0.9 \n",
      "Epoch 624   Loss 0.35324130888679595    Accuracy 0.9 \n",
      "Epoch 625   Loss 0.3532413088867736    Accuracy 0.9 \n",
      "Epoch 626   Loss 0.35324130888675187    Accuracy 0.9 \n",
      "Epoch 627   Loss 0.3532413088867307    Accuracy 0.9 \n",
      "Epoch 628   Loss 0.35324130888671024    Accuracy 0.9 \n",
      "Epoch 629   Loss 0.3532413088866902    Accuracy 0.9 \n",
      "Epoch 630   Loss 0.3532413088866709    Accuracy 0.9 \n",
      "Epoch 631   Loss 0.35324130888665195    Accuracy 0.9 \n",
      "Epoch 632   Loss 0.35324130888663363    Accuracy 0.9 \n",
      "Epoch 633   Loss 0.35324130888661576    Accuracy 0.9 \n",
      "Epoch 634   Loss 0.35324130888659855    Accuracy 0.9 \n",
      "Epoch 635   Loss 0.3532413088865816    Accuracy 0.9 \n",
      "Epoch 636   Loss 0.35324130888656524    Accuracy 0.9 \n",
      "Epoch 637   Loss 0.3532413088865493    Accuracy 0.9 \n",
      "Epoch 638   Loss 0.35324130888653393    Accuracy 0.9 \n",
      "Epoch 639   Loss 0.3532413088865187    Accuracy 0.9 \n",
      "Epoch 640   Loss 0.3532413088865042    Accuracy 0.9 \n",
      "Epoch 641   Loss 0.35324130888648997    Accuracy 0.9 \n",
      "Epoch 642   Loss 0.3532413088864762    Accuracy 0.9 \n",
      "Epoch 643   Loss 0.35324130888646266    Accuracy 0.9 \n",
      "Epoch 644   Loss 0.3532413088864496    Accuracy 0.9 \n",
      "Epoch 645   Loss 0.3532413088864369    Accuracy 0.9 \n",
      "Epoch 646   Loss 0.35324130888642463    Accuracy 0.9 \n",
      "Epoch 647   Loss 0.35324130888641253    Accuracy 0.9 \n",
      "Epoch 648   Loss 0.35324130888640093    Accuracy 0.9 \n",
      "Epoch 649   Loss 0.35324130888638955    Accuracy 0.9 \n",
      "Epoch 650   Loss 0.3532413088863785    Accuracy 0.9 \n",
      "Epoch 651   Loss 0.3532413088863678    Accuracy 0.9 \n",
      "Epoch 652   Loss 0.35324130888635735    Accuracy 0.9 \n",
      "Epoch 653   Loss 0.3532413088863472    Accuracy 0.9 \n",
      "Epoch 654   Loss 0.35324130888633737    Accuracy 0.9 \n",
      "Epoch 655   Loss 0.3532413088863278    Accuracy 0.9 \n",
      "Epoch 656   Loss 0.3532413088863185    Accuracy 0.9 \n",
      "Epoch 657   Loss 0.35324130888630945    Accuracy 0.9 \n",
      "Epoch 658   Loss 0.3532413088863006    Accuracy 0.9 \n",
      "Epoch 659   Loss 0.3532413088862921    Accuracy 0.9 \n",
      "Epoch 660   Loss 0.35324130888628374    Accuracy 0.9 \n",
      "Epoch 661   Loss 0.3532413088862757    Accuracy 0.9 \n",
      "Epoch 662   Loss 0.3532413088862678    Accuracy 0.9 \n",
      "Epoch 663   Loss 0.35324130888626015    Accuracy 0.9 \n",
      "Epoch 664   Loss 0.3532413088862528    Accuracy 0.9 \n",
      "Epoch 665   Loss 0.3532413088862456    Accuracy 0.9 \n",
      "Epoch 666   Loss 0.3532413088862385    Accuracy 0.9 \n",
      "Epoch 667   Loss 0.35324130888623173    Accuracy 0.9 \n",
      "Epoch 668   Loss 0.35324130888622507    Accuracy 0.9 \n",
      "Epoch 669   Loss 0.3532413088862186    Accuracy 0.9 \n",
      "Epoch 670   Loss 0.35324130888621236    Accuracy 0.9 \n",
      "Epoch 671   Loss 0.3532413088862063    Accuracy 0.9 \n",
      "Epoch 672   Loss 0.35324130888620026    Accuracy 0.9 \n",
      "Epoch 673   Loss 0.3532413088861946    Accuracy 0.9 \n",
      "Epoch 674   Loss 0.35324130888618893    Accuracy 0.9 \n",
      "Epoch 675   Loss 0.35324130888618355    Accuracy 0.9 \n",
      "Epoch 676   Loss 0.3532413088861782    Accuracy 0.9 \n",
      "Epoch 677   Loss 0.3532413088861731    Accuracy 0.9 \n",
      "Epoch 678   Loss 0.35324130888616817    Accuracy 0.9 \n",
      "Epoch 679   Loss 0.35324130888616323    Accuracy 0.9 \n",
      "Epoch 680   Loss 0.3532413088861585    Accuracy 0.9 \n",
      "Epoch 681   Loss 0.3532413088861539    Accuracy 0.9 \n",
      "Epoch 682   Loss 0.3532413088861494    Accuracy 0.9 \n",
      "Epoch 683   Loss 0.35324130888614513    Accuracy 0.9 \n",
      "Epoch 684   Loss 0.3532413088861409    Accuracy 0.9 \n",
      "Epoch 685   Loss 0.3532413088861368    Accuracy 0.9 \n",
      "Epoch 686   Loss 0.35324130888613275    Accuracy 0.9 \n",
      "Epoch 687   Loss 0.3532413088861289    Accuracy 0.9 \n",
      "Epoch 688   Loss 0.35324130888612515    Accuracy 0.9 \n",
      "Epoch 689   Loss 0.35324130888612154    Accuracy 0.9 \n",
      "Epoch 690   Loss 0.35324130888611793    Accuracy 0.9 \n",
      "Epoch 691   Loss 0.35324130888611444    Accuracy 0.9 \n",
      "Epoch 692   Loss 0.3532413088861111    Accuracy 0.9 \n",
      "Epoch 693   Loss 0.35324130888610783    Accuracy 0.9 \n",
      "Epoch 694   Loss 0.35324130888610467    Accuracy 0.9 \n",
      "Epoch 695   Loss 0.3532413088861015    Accuracy 0.9 \n",
      "Epoch 696   Loss 0.3532413088860985    Accuracy 0.9 \n",
      "Epoch 697   Loss 0.3532413088860956    Accuracy 0.9 \n",
      "Epoch 698   Loss 0.35324130888609273    Accuracy 0.9 \n",
      "Epoch 699   Loss 0.35324130888609    Accuracy 0.9 \n",
      "Epoch 700   Loss 0.35324130888608735    Accuracy 0.9 \n",
      "Epoch 701   Loss 0.3532413088860846    Accuracy 0.9 \n",
      "Epoch 702   Loss 0.3532413088860822    Accuracy 0.9 \n",
      "Epoch 703   Loss 0.3532413088860797    Accuracy 0.9 \n",
      "Epoch 704   Loss 0.3532413088860774    Accuracy 0.9 \n",
      "Epoch 705   Loss 0.35324130888607497    Accuracy 0.9 \n",
      "Epoch 706   Loss 0.35324130888607275    Accuracy 0.9 \n",
      "Epoch 707   Loss 0.35324130888607047    Accuracy 0.9 \n",
      "Epoch 708   Loss 0.35324130888606836    Accuracy 0.9 \n",
      "Epoch 709   Loss 0.3532413088860662    Accuracy 0.9 \n",
      "Epoch 710   Loss 0.35324130888606425    Accuracy 0.9 \n",
      "Epoch 711   Loss 0.3532413088860622    Accuracy 0.9 \n",
      "Epoch 712   Loss 0.35324130888606037    Accuracy 0.9 \n",
      "Epoch 713   Loss 0.35324130888605854    Accuracy 0.9 \n",
      "Epoch 714   Loss 0.35324130888605676    Accuracy 0.9 \n",
      "Epoch 715   Loss 0.3532413088860549    Accuracy 0.9 \n",
      "Epoch 716   Loss 0.3532413088860532    Accuracy 0.9 \n",
      "Epoch 717   Loss 0.35324130888605154    Accuracy 0.9 \n",
      "Epoch 718   Loss 0.35324130888605004    Accuracy 0.9 \n",
      "Epoch 719   Loss 0.3532413088860484    Accuracy 0.9 \n",
      "Epoch 720   Loss 0.3532413088860468    Accuracy 0.9 \n",
      "Epoch 721   Loss 0.3532413088860454    Accuracy 0.9 \n",
      "Epoch 722   Loss 0.3532413088860439    Accuracy 0.9 \n",
      "Epoch 723   Loss 0.35324130888604255    Accuracy 0.9 \n",
      "Epoch 724   Loss 0.3532413088860412    Accuracy 0.9 \n",
      "Epoch 725   Loss 0.3532413088860399    Accuracy 0.9 \n",
      "Epoch 726   Loss 0.35324130888603855    Accuracy 0.9 \n",
      "Epoch 727   Loss 0.35324130888603733    Accuracy 0.9 \n",
      "Epoch 728   Loss 0.3532413088860361    Accuracy 0.9 \n",
      "Epoch 729   Loss 0.35324130888603494    Accuracy 0.9 \n",
      "Epoch 730   Loss 0.3532413088860337    Accuracy 0.9 \n",
      "Epoch 731   Loss 0.3532413088860326    Accuracy 0.9 \n",
      "Epoch 732   Loss 0.3532413088860315    Accuracy 0.9 \n",
      "Epoch 733   Loss 0.3532413088860305    Accuracy 0.9 \n",
      "Epoch 734   Loss 0.35324130888602945    Accuracy 0.9 \n",
      "Epoch 735   Loss 0.35324130888602845    Accuracy 0.9 \n",
      "Epoch 736   Loss 0.3532413088860275    Accuracy 0.9 \n",
      "Epoch 737   Loss 0.35324130888602645    Accuracy 0.9 \n",
      "Epoch 738   Loss 0.3532413088860256    Accuracy 0.9 \n",
      "Epoch 739   Loss 0.3532413088860248    Accuracy 0.9 \n",
      "Epoch 740   Loss 0.3532413088860239    Accuracy 0.9 \n",
      "Epoch 741   Loss 0.353241308886023    Accuracy 0.9 \n",
      "Epoch 742   Loss 0.35324130888602223    Accuracy 0.9 \n",
      "Epoch 743   Loss 0.3532413088860214    Accuracy 0.9 \n",
      "Epoch 744   Loss 0.35324130888602057    Accuracy 0.9 \n",
      "Epoch 745   Loss 0.35324130888601984    Accuracy 0.9 \n",
      "Epoch 746   Loss 0.3532413088860191    Accuracy 0.9 \n",
      "Epoch 747   Loss 0.3532413088860184    Accuracy 0.9 \n",
      "Epoch 748   Loss 0.35324130888601774    Accuracy 0.9 \n",
      "Epoch 749   Loss 0.35324130888601707    Accuracy 0.9 \n",
      "Epoch 750   Loss 0.3532413088860164    Accuracy 0.9 \n",
      "Epoch 751   Loss 0.3532413088860158    Accuracy 0.9 \n",
      "Epoch 752   Loss 0.3532413088860151    Accuracy 0.9 \n",
      "Epoch 753   Loss 0.3532413088860145    Accuracy 0.9 \n",
      "Epoch 754   Loss 0.35324130888601396    Accuracy 0.9 \n",
      "Epoch 755   Loss 0.35324130888601346    Accuracy 0.9 \n",
      "Epoch 756   Loss 0.3532413088860128    Accuracy 0.9 \n",
      "Epoch 757   Loss 0.35324130888601235    Accuracy 0.9 \n",
      "Epoch 758   Loss 0.35324130888601174    Accuracy 0.9 \n",
      "Epoch 759   Loss 0.35324130888601124    Accuracy 0.9 \n",
      "Epoch 760   Loss 0.3532413088860108    Accuracy 0.9 \n",
      "Epoch 761   Loss 0.35324130888601024    Accuracy 0.9 \n",
      "Epoch 762   Loss 0.35324130888600985    Accuracy 0.9 \n",
      "Epoch 763   Loss 0.3532413088860094    Accuracy 0.9 \n",
      "Epoch 764   Loss 0.3532413088860089    Accuracy 0.9 \n",
      "Epoch 765   Loss 0.3532413088860085    Accuracy 0.9 \n",
      "Epoch 766   Loss 0.3532413088860081    Accuracy 0.9 \n",
      "Epoch 767   Loss 0.3532413088860077    Accuracy 0.9 \n",
      "Epoch 768   Loss 0.35324130888600724    Accuracy 0.9 \n",
      "Epoch 769   Loss 0.35324130888600686    Accuracy 0.9 \n",
      "Epoch 770   Loss 0.3532413088860065    Accuracy 0.9 \n",
      "Epoch 771   Loss 0.35324130888600613    Accuracy 0.9 \n",
      "Epoch 772   Loss 0.35324130888600574    Accuracy 0.9 \n",
      "Epoch 773   Loss 0.35324130888600547    Accuracy 0.9 \n",
      "Epoch 774   Loss 0.35324130888600513    Accuracy 0.9 \n",
      "Epoch 775   Loss 0.3532413088860048    Accuracy 0.9 \n",
      "Epoch 776   Loss 0.3532413088860045    Accuracy 0.9 \n",
      "Epoch 777   Loss 0.35324130888600425    Accuracy 0.9 \n",
      "Epoch 778   Loss 0.3532413088860039    Accuracy 0.9 \n",
      "Epoch 779   Loss 0.3532413088860037    Accuracy 0.9 \n",
      "Epoch 780   Loss 0.35324130888600336    Accuracy 0.9 \n",
      "Epoch 781   Loss 0.353241308886003    Accuracy 0.9 \n",
      "Epoch 782   Loss 0.3532413088860027    Accuracy 0.9 \n",
      "Epoch 783   Loss 0.3532413088860026    Accuracy 0.9 \n",
      "Epoch 784   Loss 0.3532413088860023    Accuracy 0.9 \n",
      "Epoch 785   Loss 0.3532413088860021    Accuracy 0.9 \n",
      "Epoch 786   Loss 0.3532413088860019    Accuracy 0.9 \n",
      "Epoch 787   Loss 0.3532413088860015    Accuracy 0.9 \n",
      "Epoch 788   Loss 0.35324130888600136    Accuracy 0.9 \n",
      "Epoch 789   Loss 0.3532413088860012    Accuracy 0.9 \n",
      "Epoch 790   Loss 0.3532413088860009    Accuracy 0.9 \n",
      "Epoch 791   Loss 0.35324130888600075    Accuracy 0.9 \n",
      "Epoch 792   Loss 0.3532413088860005    Accuracy 0.9 \n",
      "Epoch 793   Loss 0.35324130888600036    Accuracy 0.9 \n",
      "Epoch 794   Loss 0.35324130888600014    Accuracy 0.9 \n",
      "Epoch 795   Loss 0.3532413088859999    Accuracy 0.9 \n",
      "Epoch 796   Loss 0.3532413088859998    Accuracy 0.9 \n",
      "Epoch 797   Loss 0.3532413088859996    Accuracy 0.9 \n",
      "Epoch 798   Loss 0.35324130888599947    Accuracy 0.9 \n",
      "Epoch 799   Loss 0.3532413088859993    Accuracy 0.9 \n",
      "Epoch 800   Loss 0.3532413088859991    Accuracy 0.9 \n",
      "Epoch 801   Loss 0.3532413088859989    Accuracy 0.9 \n",
      "Epoch 802   Loss 0.35324130888599886    Accuracy 0.9 \n",
      "Epoch 803   Loss 0.3532413088859986    Accuracy 0.9 \n",
      "Epoch 804   Loss 0.35324130888599853    Accuracy 0.9 \n",
      "Epoch 805   Loss 0.35324130888599836    Accuracy 0.9 \n",
      "Epoch 806   Loss 0.35324130888599825    Accuracy 0.9 \n",
      "Epoch 807   Loss 0.35324130888599814    Accuracy 0.9 \n",
      "Epoch 808   Loss 0.35324130888599803    Accuracy 0.9 \n",
      "Epoch 809   Loss 0.35324130888599786    Accuracy 0.9 \n",
      "Epoch 810   Loss 0.35324130888599775    Accuracy 0.9 \n",
      "Epoch 811   Loss 0.35324130888599764    Accuracy 0.9 \n",
      "Epoch 812   Loss 0.3532413088859975    Accuracy 0.9 \n",
      "Epoch 813   Loss 0.3532413088859975    Accuracy 0.9 \n",
      "Epoch 814   Loss 0.35324130888599725    Accuracy 0.9 \n",
      "Epoch 815   Loss 0.35324130888599714    Accuracy 0.9 \n",
      "Epoch 816   Loss 0.3532413088859971    Accuracy 0.9 \n",
      "Epoch 817   Loss 0.35324130888599703    Accuracy 0.9 \n",
      "Epoch 818   Loss 0.3532413088859969    Accuracy 0.9 \n",
      "Epoch 819   Loss 0.3532413088859968    Accuracy 0.9 \n",
      "Epoch 820   Loss 0.35324130888599664    Accuracy 0.9 \n",
      "Epoch 821   Loss 0.3532413088859966    Accuracy 0.9 \n",
      "Epoch 822   Loss 0.35324130888599653    Accuracy 0.9 \n",
      "Epoch 823   Loss 0.3532413088859965    Accuracy 0.9 \n",
      "Epoch 824   Loss 0.35324130888599636    Accuracy 0.9 \n",
      "Epoch 825   Loss 0.3532413088859963    Accuracy 0.9 \n",
      "Epoch 826   Loss 0.35324130888599614    Accuracy 0.9 \n",
      "Epoch 827   Loss 0.35324130888599614    Accuracy 0.9 \n",
      "Epoch 828   Loss 0.3532413088859961    Accuracy 0.9 \n",
      "Epoch 829   Loss 0.353241308885996    Accuracy 0.9 \n",
      "Epoch 830   Loss 0.3532413088859959    Accuracy 0.9 \n",
      "Epoch 831   Loss 0.35324130888599586    Accuracy 0.9 \n",
      "Epoch 832   Loss 0.35324130888599586    Accuracy 0.9 \n",
      "Epoch 833   Loss 0.3532413088859957    Accuracy 0.9 \n",
      "Epoch 834   Loss 0.3532413088859957    Accuracy 0.9 \n",
      "Epoch 835   Loss 0.35324130888599564    Accuracy 0.9 \n",
      "Epoch 836   Loss 0.3532413088859956    Accuracy 0.9 \n",
      "Epoch 837   Loss 0.3532413088859955    Accuracy 0.9 \n",
      "Epoch 838   Loss 0.3532413088859955    Accuracy 0.9 \n",
      "Epoch 839   Loss 0.3532413088859954    Accuracy 0.9 \n",
      "Epoch 840   Loss 0.3532413088859953    Accuracy 0.9 \n",
      "Epoch 841   Loss 0.3532413088859953    Accuracy 0.9 \n",
      "Epoch 842   Loss 0.35324130888599525    Accuracy 0.9 \n",
      "Epoch 843   Loss 0.35324130888599525    Accuracy 0.9 \n",
      "Epoch 844   Loss 0.35324130888599514    Accuracy 0.9 \n",
      "Epoch 845   Loss 0.35324130888599514    Accuracy 0.9 \n",
      "Epoch 846   Loss 0.35324130888599503    Accuracy 0.9 \n",
      "Epoch 847   Loss 0.35324130888599503    Accuracy 0.9 \n",
      "Epoch 848   Loss 0.35324130888599503    Accuracy 0.9 \n",
      "Epoch 849   Loss 0.353241308885995    Accuracy 0.9 \n",
      "Epoch 850   Loss 0.3532413088859949    Accuracy 0.9 \n",
      "Epoch 851   Loss 0.3532413088859948    Accuracy 0.9 \n",
      "Epoch 852   Loss 0.35324130888599486    Accuracy 0.9 \n",
      "Epoch 853   Loss 0.3532413088859948    Accuracy 0.9 \n",
      "Epoch 854   Loss 0.3532413088859948    Accuracy 0.9 \n",
      "Epoch 855   Loss 0.35324130888599475    Accuracy 0.9 \n",
      "Epoch 856   Loss 0.35324130888599464    Accuracy 0.9 \n",
      "Epoch 857   Loss 0.35324130888599464    Accuracy 0.9 \n",
      "Epoch 858   Loss 0.35324130888599464    Accuracy 0.9 \n",
      "Epoch 859   Loss 0.3532413088859946    Accuracy 0.9 \n",
      "Epoch 860   Loss 0.3532413088859946    Accuracy 0.9 \n",
      "Epoch 861   Loss 0.35324130888599453    Accuracy 0.9 \n",
      "Epoch 862   Loss 0.3532413088859945    Accuracy 0.9 \n",
      "Epoch 863   Loss 0.3532413088859946    Accuracy 0.9 \n",
      "Epoch 864   Loss 0.35324130888599453    Accuracy 0.9 \n",
      "Epoch 865   Loss 0.3532413088859944    Accuracy 0.9 \n",
      "Epoch 866   Loss 0.3532413088859944    Accuracy 0.9 \n",
      "Epoch 867   Loss 0.3532413088859944    Accuracy 0.9 \n",
      "Epoch 868   Loss 0.35324130888599437    Accuracy 0.9 \n",
      "Epoch 869   Loss 0.35324130888599437    Accuracy 0.9 \n",
      "Epoch 870   Loss 0.3532413088859943    Accuracy 0.9 \n",
      "Epoch 871   Loss 0.3532413088859943    Accuracy 0.9 \n",
      "Epoch 872   Loss 0.3532413088859943    Accuracy 0.9 \n",
      "Epoch 873   Loss 0.35324130888599425    Accuracy 0.9 \n",
      "Epoch 874   Loss 0.3532413088859943    Accuracy 0.9 \n",
      "Epoch 875   Loss 0.35324130888599425    Accuracy 0.9 \n",
      "Epoch 876   Loss 0.35324130888599414    Accuracy 0.9 \n",
      "Epoch 877   Loss 0.3532413088859942    Accuracy 0.9 \n",
      "Epoch 878   Loss 0.35324130888599425    Accuracy 0.9 \n",
      "Epoch 879   Loss 0.35324130888599414    Accuracy 0.9 \n",
      "Epoch 880   Loss 0.35324130888599414    Accuracy 0.9 \n",
      "Epoch 881   Loss 0.35324130888599414    Accuracy 0.9 \n",
      "Epoch 882   Loss 0.3532413088859941    Accuracy 0.9 \n",
      "Epoch 883   Loss 0.35324130888599414    Accuracy 0.9 \n",
      "Epoch 884   Loss 0.35324130888599403    Accuracy 0.9 \n",
      "Epoch 885   Loss 0.3532413088859941    Accuracy 0.9 \n",
      "Epoch 886   Loss 0.35324130888599403    Accuracy 0.9 \n",
      "Epoch 887   Loss 0.353241308885994    Accuracy 0.9 \n",
      "Epoch 888   Loss 0.353241308885994    Accuracy 0.9 \n",
      "Epoch 889   Loss 0.353241308885994    Accuracy 0.9 \n",
      "Epoch 890   Loss 0.353241308885994    Accuracy 0.9 \n",
      "Epoch 891   Loss 0.353241308885994    Accuracy 0.9 \n",
      "Epoch 892   Loss 0.353241308885994    Accuracy 0.9 \n",
      "Epoch 893   Loss 0.3532413088859939    Accuracy 0.9 \n",
      "Epoch 894   Loss 0.3532413088859939    Accuracy 0.9 \n",
      "Epoch 895   Loss 0.3532413088859939    Accuracy 0.9 \n",
      "Epoch 896   Loss 0.353241308885994    Accuracy 0.9 \n",
      "Epoch 897   Loss 0.35324130888599387    Accuracy 0.9 \n",
      "Epoch 898   Loss 0.3532413088859939    Accuracy 0.9 \n",
      "Epoch 899   Loss 0.3532413088859939    Accuracy 0.9 \n",
      "Epoch 900   Loss 0.35324130888599387    Accuracy 0.9 \n",
      "Epoch 901   Loss 0.3532413088859938    Accuracy 0.9 \n",
      "Epoch 902   Loss 0.3532413088859938    Accuracy 0.9 \n",
      "Epoch 903   Loss 0.3532413088859939    Accuracy 0.9 \n",
      "Epoch 904   Loss 0.35324130888599387    Accuracy 0.9 \n",
      "Epoch 905   Loss 0.3532413088859939    Accuracy 0.9 \n",
      "Epoch 906   Loss 0.3532413088859938    Accuracy 0.9 \n",
      "Epoch 907   Loss 0.3532413088859938    Accuracy 0.9 \n",
      "Epoch 908   Loss 0.3532413088859939    Accuracy 0.9 \n",
      "Epoch 909   Loss 0.35324130888599375    Accuracy 0.9 \n",
      "Epoch 910   Loss 0.3532413088859937    Accuracy 0.9 \n",
      "Epoch 911   Loss 0.3532413088859938    Accuracy 0.9 \n",
      "Epoch 912   Loss 0.35324130888599375    Accuracy 0.9 \n",
      "Epoch 913   Loss 0.3532413088859938    Accuracy 0.9 \n",
      "Epoch 914   Loss 0.35324130888599375    Accuracy 0.9 \n",
      "Epoch 915   Loss 0.35324130888599375    Accuracy 0.9 \n",
      "Epoch 916   Loss 0.3532413088859938    Accuracy 0.9 \n",
      "Epoch 917   Loss 0.35324130888599375    Accuracy 0.9 \n",
      "Epoch 918   Loss 0.3532413088859937    Accuracy 0.9 \n",
      "Epoch 919   Loss 0.35324130888599375    Accuracy 0.9 \n",
      "Epoch 920   Loss 0.3532413088859937    Accuracy 0.9 \n",
      "Epoch 921   Loss 0.3532413088859937    Accuracy 0.9 \n",
      "Epoch 922   Loss 0.35324130888599375    Accuracy 0.9 \n",
      "Epoch 923   Loss 0.35324130888599375    Accuracy 0.9 \n",
      "Epoch 924   Loss 0.35324130888599364    Accuracy 0.9 \n",
      "Epoch 925   Loss 0.3532413088859937    Accuracy 0.9 \n",
      "Epoch 926   Loss 0.3532413088859937    Accuracy 0.9 \n",
      "Epoch 927   Loss 0.3532413088859937    Accuracy 0.9 \n",
      "Epoch 928   Loss 0.3532413088859937    Accuracy 0.9 \n",
      "Epoch 929   Loss 0.3532413088859937    Accuracy 0.9 \n",
      "Epoch 930   Loss 0.3532413088859937    Accuracy 0.9 \n",
      "Epoch 931   Loss 0.3532413088859937    Accuracy 0.9 \n",
      "Epoch 932   Loss 0.3532413088859937    Accuracy 0.9 \n",
      "Epoch 933   Loss 0.3532413088859937    Accuracy 0.9 \n",
      "Epoch 934   Loss 0.3532413088859937    Accuracy 0.9 \n",
      "Epoch 935   Loss 0.3532413088859937    Accuracy 0.9 \n",
      "Epoch 936   Loss 0.3532413088859937    Accuracy 0.9 \n",
      "Epoch 937   Loss 0.35324130888599364    Accuracy 0.9 \n",
      "Epoch 938   Loss 0.3532413088859937    Accuracy 0.9 \n",
      "Epoch 939   Loss 0.35324130888599364    Accuracy 0.9 \n",
      "Epoch 940   Loss 0.3532413088859937    Accuracy 0.9 \n",
      "Epoch 941   Loss 0.3532413088859937    Accuracy 0.9 \n",
      "Epoch 942   Loss 0.3532413088859937    Accuracy 0.9 \n",
      "Epoch 943   Loss 0.3532413088859937    Accuracy 0.9 \n",
      "Epoch 944   Loss 0.3532413088859937    Accuracy 0.9 \n",
      "Epoch 945   Loss 0.3532413088859937    Accuracy 0.9 \n",
      "Epoch 946   Loss 0.3532413088859937    Accuracy 0.9 \n",
      "Epoch 947   Loss 0.35324130888599364    Accuracy 0.9 \n",
      "Epoch 948   Loss 0.35324130888599364    Accuracy 0.9 \n",
      "Epoch 949   Loss 0.35324130888599364    Accuracy 0.9 \n",
      "Epoch 950   Loss 0.35324130888599364    Accuracy 0.9 \n",
      "Epoch 951   Loss 0.3532413088859937    Accuracy 0.9 \n",
      "Epoch 952   Loss 0.3532413088859936    Accuracy 0.9 \n",
      "Epoch 953   Loss 0.35324130888599364    Accuracy 0.9 \n",
      "Epoch 954   Loss 0.35324130888599364    Accuracy 0.9 \n",
      "Epoch 955   Loss 0.3532413088859936    Accuracy 0.9 \n",
      "Epoch 956   Loss 0.3532413088859937    Accuracy 0.9 \n",
      "Epoch 957   Loss 0.35324130888599364    Accuracy 0.9 \n",
      "Epoch 958   Loss 0.3532413088859936    Accuracy 0.9 \n",
      "Epoch 959   Loss 0.35324130888599364    Accuracy 0.9 \n",
      "Epoch 960   Loss 0.3532413088859936    Accuracy 0.9 \n",
      "Epoch 961   Loss 0.3532413088859936    Accuracy 0.9 \n",
      "Epoch 962   Loss 0.3532413088859936    Accuracy 0.9 \n",
      "Epoch 963   Loss 0.35324130888599364    Accuracy 0.9 \n",
      "Epoch 964   Loss 0.35324130888599364    Accuracy 0.9 \n",
      "Epoch 965   Loss 0.35324130888599364    Accuracy 0.9 \n",
      "Epoch 966   Loss 0.3532413088859936    Accuracy 0.9 \n",
      "Epoch 967   Loss 0.3532413088859936    Accuracy 0.9 \n",
      "Epoch 968   Loss 0.35324130888599353    Accuracy 0.9 \n",
      "Epoch 969   Loss 0.3532413088859936    Accuracy 0.9 \n",
      "Epoch 970   Loss 0.3532413088859936    Accuracy 0.9 \n",
      "Epoch 971   Loss 0.3532413088859936    Accuracy 0.9 \n",
      "Epoch 972   Loss 0.3532413088859936    Accuracy 0.9 \n",
      "Epoch 973   Loss 0.3532413088859936    Accuracy 0.9 \n",
      "Epoch 974   Loss 0.35324130888599364    Accuracy 0.9 \n",
      "Epoch 975   Loss 0.3532413088859936    Accuracy 0.9 \n",
      "Epoch 976   Loss 0.3532413088859936    Accuracy 0.9 \n",
      "Epoch 977   Loss 0.3532413088859936    Accuracy 0.9 \n",
      "Epoch 978   Loss 0.35324130888599364    Accuracy 0.9 \n",
      "Epoch 979   Loss 0.3532413088859936    Accuracy 0.9 \n",
      "Epoch 980   Loss 0.35324130888599353    Accuracy 0.9 \n",
      "Epoch 981   Loss 0.35324130888599364    Accuracy 0.9 \n",
      "Epoch 982   Loss 0.35324130888599353    Accuracy 0.9 \n",
      "Epoch 983   Loss 0.35324130888599353    Accuracy 0.9 \n",
      "Epoch 984   Loss 0.35324130888599364    Accuracy 0.9 \n",
      "Epoch 985   Loss 0.3532413088859936    Accuracy 0.9 \n",
      "Epoch 986   Loss 0.3532413088859936    Accuracy 0.9 \n",
      "Epoch 987   Loss 0.3532413088859936    Accuracy 0.9 \n",
      "Epoch 988   Loss 0.3532413088859936    Accuracy 0.9 \n",
      "Epoch 989   Loss 0.35324130888599364    Accuracy 0.9 \n",
      "Epoch 990   Loss 0.3532413088859936    Accuracy 0.9 \n",
      "Epoch 991   Loss 0.3532413088859936    Accuracy 0.9 \n",
      "Epoch 992   Loss 0.3532413088859936    Accuracy 0.9 \n",
      "Epoch 993   Loss 0.3532413088859936    Accuracy 0.9 \n",
      "Epoch 994   Loss 0.35324130888599353    Accuracy 0.9 \n",
      "Epoch 995   Loss 0.35324130888599353    Accuracy 0.9 \n",
      "Epoch 996   Loss 0.35324130888599364    Accuracy 0.9 \n",
      "Epoch 997   Loss 0.35324130888599353    Accuracy 0.9 \n",
      "Epoch 998   Loss 0.35324130888599353    Accuracy 0.9 \n",
      "Epoch 999   Loss 0.3532413088859936    Accuracy 0.9 \n",
      "Epoch 1000   Loss 0.3532413088859936    Accuracy 0.9 \n"
     ]
    }
   ],
   "source": [
    "trainX , trainY , testX , testY = x[:20],target[:20],x[20:],target[20:]\n",
    "# rate = 0.1, threshold = 0.01 , epochs = 10\n",
    "weight , bias = gradient_descent(trainX,trainY,1000,0.3,10.8) # Increased learning rate from 0.001 to 0.1\n",
    "\n",
    "#Accuracy is observed to be improved after Standard Scaling (Value-minimum)/Range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 181,
     "status": "ok",
     "timestamp": 1767374042383,
     "user": {
      "displayName": "Rajdip Sinha",
      "userId": "10072767020088473354"
     },
     "user_tz": -330
    },
    "id": "dahHpbX4gUqT",
    "outputId": "994dfe38-2fa8-4435-ebe8-ce4e7c1183cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87.5%\n"
     ]
    }
   ],
   "source": [
    "n = Node(weight,bias)\n",
    "predictions = n.prediction(testX)\n",
    "print(f\"{accuracy_measurement(predictions,testY)*100}%\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNxx0MNAjuW4FZ4bK9U7BDF",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
