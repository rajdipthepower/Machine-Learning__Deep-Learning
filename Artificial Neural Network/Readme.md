Neural Network from ScratchThis project implements a single-layer neural network (a single neuron) from the ground up using Python and NumPy. It covers the mathematical derivation of backpropagation, vectorized implementation, and practical application to a binary classification problem.ðŸš€ OverviewThe notebook demonstrates how a single neuron learns to make binary predictions (Binary Cross Entropy) using Gradient Descent. It avoids high-level machine learning libraries like Scikit-Learn or TensorFlow for the core logic to provide a deep understanding of the underlying mechanics.ðŸ§  Mathematical FoundationsThe implementation is based on the following mathematical concepts, which are detailed in the notebook:1. Forward PropagationLinear Combination ($z$): $z = w_1x_1 + w_2x_2 + b$Activation ($a$): Sigmoid function $\sigma(z) = \frac{1}{1 + e^{-z}}$Loss Function ($L$): Log Loss (Binary Cross Entropy)2. Backpropagation (The Chain Rule)The notebook provides a step-by-step derivation to find the gradient of the loss with respect to the weights:$$\frac{\partial L}{\partial w_1} = (a - y)x_1$$Where $a$ is the prediction and $y$ is the true label.3. VectorizationCalculations are optimized using NumPy matrix operations. The mean gradient across $n$ samples is calculated as:$$\frac{\partial J}{\partial w} = \frac{1}{n} X^T (A - Y)$$Dividing by $n$ ensures the "adjustment signal" remains stable regardless of dataset size.ðŸ’» Code Structuresigmoid(input): Helper function to map values between 0 and 1.Node Class: Represents the neuron, containing weights and bias, with methods for output calculation and prediction.logloss(y_true, y_predic): Implementation of Binary Cross Entropy with epsilon-clipping to prevent logarithmic divergence.gradient_descent(x, y_true, epochs, thres, rate): The core training loop that updates weights and bias iteratively.ðŸ“Š ResultsThe model was tested on an insurance dataset (predicting bought_insurance based on age and affordability). After standard scaling and training with a learning rate of 0.3 for 1000 epochs, the model achieved:Final Accuracy: 87.5%.ðŸ›  RequirementsNumPyPandasMatplotlibðŸ“– UsageLoad your dataset into a Pandas DataFrame.Preprocess features (e.g., Standard Scaling).Call gradient_descent(X, y, epochs, threshold, learning_rate) to train the model.Use the returned weights and bias to initialize a Node for future predictions.
